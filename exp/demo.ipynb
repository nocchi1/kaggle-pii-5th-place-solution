{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../config/exp_087.yaml\n",
    "# exp: \"087\"\n",
    "# debug: false\n",
    "# seed: 10\n",
    "# task_type: \"detect\"\n",
    "# device: \"cuda\"\n",
    "\n",
    "# # data preprocess\n",
    "# remove_prefix: true\n",
    "# exter_dataset:\n",
    "#   - [\"nicholas\", true]\n",
    "#   - [\"mpware\", false]\n",
    "#   - [\"pjma\", false]\n",
    "\n",
    "# n_fold: 3\n",
    "# use_fold: 3\n",
    "\n",
    "# # dataset, dataloader\n",
    "# add_newline_token: true\n",
    "# max_length: 128\n",
    "# train_stride: 96\n",
    "# eval_stride: 64\n",
    "# train_batch: 16\n",
    "# eval_batch: 64\n",
    "\n",
    "# # model\n",
    "# model_path: \"microsoft/deberta-v3-large\"\n",
    "# class_num: 8 # with prefix -> 13, without prefix -> 8\n",
    "# lstm_type: \"none\"\n",
    "# use_hidden_states: 2\n",
    "# dropout: 0.10\n",
    "# hidden_dropout: 0.10\n",
    "# attention_dropout: 0.10\n",
    "# reinit_layer_num: 0\n",
    "# freeze_layer_num: 0\n",
    "\n",
    "# # loss\n",
    "# smooth_type: \"online\"\n",
    "# smooth_ratio: 0.05\n",
    "# smooth_pair: 0.05\n",
    "# positive_class_weight: 10\n",
    "\n",
    "# # optimizer\n",
    "# optimizer_type: \"AdamW\"\n",
    "# pretrained_lr: 1e-6\n",
    "# head_lr: 1e-4\n",
    "# weight_decay: 0.01\n",
    "# betas: [0.9, 0.999]\n",
    "\n",
    "# # scheduler\n",
    "# scheduler_type: \"cosine_custom\"\n",
    "# first_cycle_epochs: 4\n",
    "# cycle_factor: 1\n",
    "# num_warmup_steps: 0\n",
    "# min_lr: 1e-9\n",
    "# gamma: 1.0\n",
    "\n",
    "# # training\n",
    "# epochs: 4\n",
    "# accumulation_steps: 2\n",
    "# eval_steps: 1000\n",
    "# negative_th: 0.660\n",
    "# negative_th_method: \"overall\"\n",
    "# amp: true\n",
    "# ema: false\n",
    "# ema_decay: 0.999\n",
    "# ema_update_after_step: 8000\n",
    "\n",
    "# # additional training\n",
    "# add_train: true\n",
    "# add_epochs: 4\n",
    "# add_first_cycle_epochs: 4\n",
    "\n",
    "# # full training\n",
    "# full_train: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.preprocess import DetectDataProvider\n",
    "from src.train import get_full_train_loader, get_train_loaders\n",
    "from src.train.dataloader_utils import CollateFn, get_sampler, get_tokenizer\n",
    "from src.utils import TimeUtil, get_config, get_logger, seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]コマンドライン引数\n",
    "exp = \"087\"\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-19 23:30:25\u001b[0m | \u001b[1mINFO ] exp:087 start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path(\"../config\"))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f\"exp:{exp} start\")\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n",
    "config.debug = debug\n",
    "config.use_fold = 3\n",
    "config.eval_steps = 500  # 100\n",
    "config.ema_update_after_step = 100\n",
    "\n",
    "config.epochs = 2\n",
    "config.first_cycle_epochs = 2\n",
    "config.add_epochs = 2\n",
    "config.add_first_cycle_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-19 16:30:46\u001b[0m | \u001b[1mINFO ] Data Size: 13854\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dpr = DetectDataProvider(config, \"train\")\n",
    "data = dpr.load_data()\n",
    "logger.info(f\"Data Size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [TODO]データサイズを調整する\n",
    "\n",
    "data_ = []\n",
    "for fold in [-1, 0, 1, 2]:\n",
    "    fold_data = [d for d in data if d[\"fold\"] == fold]\n",
    "    fold_data = fold_data[:100]\n",
    "    data_.extend(fold_data)\n",
    "\n",
    "data = data_\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = get_train_loaders(config, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loguru\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from torch.cuda import amp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.train.component_factory import ComponentFactory\n",
    "from src.train.ema import ModelEmaV3\n",
    "from src.utils.competition_utils import (\n",
    "    get_char2org_df,\n",
    "    get_char_pred_df,\n",
    "    get_original_token_df,\n",
    "    get_pred_df,\n",
    "    get_truth_df,\n",
    "    restore_prefix,\n",
    ")\n",
    "from src.utils.metric import evaluate_metric, get_best_negative_threshold\n",
    "from src.utils.utils import AverageMeter, clean_message\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "\n",
    "        if self.config.ema:\n",
    "            self.model_ema = ModelEmaV3(\n",
    "                self.model,\n",
    "                decay=config.ema_decay,\n",
    "                update_after_step=config.ema_update_after_step,\n",
    "                device=config.device,\n",
    "            )\n",
    "\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.grad_scaler = amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader | None,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        full_train: bool = False,\n",
    "        full_steps: int = 0,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            assert not full_train, \"eval_only and full_train cannot be True at the same time\"\n",
    "            score, loss, oof_df = self.valid_evaluate(valid_loader, epoch=-1, load_best_weight=True)\n",
    "            return score, -1, oof_df\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(self.config, self.model)\n",
    "\n",
    "        global_steps = 0\n",
    "        update_steps = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.model.load_state_dict(torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\"))\n",
    "            self.model_ema.update_after_step = 0\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        # 学習ループの開始\n",
    "        epochs = self.config.epochs if not retrain else self.config.add_epochs\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            # 1epoch目はbackboneをfreezeする\n",
    "            if epoch == 0 and not retrain:\n",
    "                self.model.freeze_backbone(config.reinit_layer_num)\n",
    "            elif epoch == 1 and not retrain:\n",
    "                self.model.unfreeze_backbone(config.freeze_layer_num)\n",
    "\n",
    "            iterations = tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(self.model, data)\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                loss = loss / self.config.accumulation_steps\n",
    "                self.grad_scaler.scale(loss).backward()\n",
    "                global_steps += 1\n",
    "\n",
    "                if global_steps % self.config.accumulation_steps == 0:\n",
    "                    self.grad_scaler.step(self.optimizer)\n",
    "                    self.grad_scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    update_steps += 1\n",
    "\n",
    "                    if self.config.ema:\n",
    "                        self.model_ema.update(self.model, update_steps)\n",
    "\n",
    "                    # backboneの学習が始まってからschedulerを適用\n",
    "                    if epoch >= 1 or retrain:\n",
    "                        if self.scheduler is None:\n",
    "                            first_cycle_epochs = (\n",
    "                                self.config.first_cycle_epochs if not retrain else self.config.add_first_cycle_epochs\n",
    "                            )\n",
    "                            total_steps = first_cycle_epochs * len(train_loader)\n",
    "                            if not retrain:\n",
    "                                total_steps -= len(train_loader)  # 最初の1epoch分はstepしないから\n",
    "                            self.scheduler = ComponentFactory.get_scheduler(\n",
    "                                self.config, self.optimizer, total_steps=total_steps\n",
    "                            )\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                if global_steps % self.config.eval_steps == 0 and not full_train:\n",
    "                    score, loss, oof_df = self.valid_evaluate(valid_loader, epoch, load_best_weight=False)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_steps = global_steps\n",
    "                        best_oof = oof_df\n",
    "                        parameters = self.model_ema.module.state_dict() if self.config.ema else self.model.state_dict()\n",
    "                        torch.save(\n",
    "                            parameters,\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "                    self.model.train()\n",
    "\n",
    "                if full_train and global_steps >= full_steps:\n",
    "                    parameters = self.model_ema.module.state_dict() if self.config.ema else self.model.state_dict()\n",
    "                    torch.save(\n",
    "                        parameters,\n",
    "                        self.config.output_path / f\"model{self.save_suffix}_full.pth\",\n",
    "                    )\n",
    "                    return None\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "            if self.config.smooth_type == \"online\":\n",
    "                self.loss_fn.update_soft_matrix()\n",
    "\n",
    "        return best_score, best_steps, best_oof\n",
    "\n",
    "    def valid_evaluate(self, valid_loader: DataLoader, epoch: int, load_best_weight: bool = False):\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\"))\n",
    "\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = tqdm(valid_loader, total=len(valid_loader)) if self.detail_pbar else valid_loader\n",
    "            for data in iterations:\n",
    "                if load_best_weight or not self.config.ema:\n",
    "                    out, loss = self.forward_step(self.model, data)\n",
    "                else:\n",
    "                    out, loss = self.forward_step(self.model_ema, data)\n",
    "\n",
    "                self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                preds.extend(F.softmax(out, dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "        oof_df = self.get_oof_df(preds, valid_loader)\n",
    "        score, best_th = get_best_negative_threshold(self.config, oof_df)\n",
    "\n",
    "        loss = self.valid_loss.avg\n",
    "        message = f\"\"\"\n",
    "            Valid :\n",
    "                Epoch={epoch},\n",
    "                Loss={loss:.5f},\n",
    "                Score={score:.5f}\n",
    "                Threshold={best_th}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, loss, oof_df\n",
    "\n",
    "    def forward_step(self, model: nn.Module, data: torch.Tensor):\n",
    "        input_ids, attention_mask, positions_feats, labels = data\n",
    "        input_ids = input_ids.to(self.config.device)\n",
    "        attention_mask = attention_mask.to(self.config.device)\n",
    "        positions_feats = positions_feats.to(self.config.device)\n",
    "        labels = labels.to(self.config.device)\n",
    "\n",
    "        with amp.autocast(enabled=self.config.amp):\n",
    "            out = model(input_ids, attention_mask, positions_feats)\n",
    "            loss = self.loss_fn(out, labels)\n",
    "        return out, loss\n",
    "\n",
    "    def get_oof_df(self, preds: list[list[float]], valid_loader: DataLoader):\n",
    "        char_pred_df = get_char_pred_df(\n",
    "            preds,\n",
    "            valid_loader.dataset.overlap_doc_ids,\n",
    "            valid_loader.dataset.offset_mapping,\n",
    "            class_num=self.config.class_num,\n",
    "        )\n",
    "        char2org_df = get_char2org_df(\n",
    "            valid_loader.dataset.doc_ids,\n",
    "            valid_loader.dataset.full_texts,\n",
    "            valid_loader.dataset.org_tokens,\n",
    "            valid_loader.dataset.whitespaces,\n",
    "        )\n",
    "        oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\", coalesce=True)\n",
    "        oof_df = (\n",
    "            oof_df.filter(pl.col(\"token_idx\") != -1)\n",
    "            .group_by(\"document\", \"token_idx\")\n",
    "            .agg([pl.col(f\"pred_{i}\").mean() for i in range(self.config.class_num)])\n",
    "        )\n",
    "        return oof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_dfs = []\n",
    "# best_steps, best_add_steps = [], []\n",
    "# collate_fn = CollateFn(get_tokenizer(config), is_train=True)\n",
    "\n",
    "# # この学習でベストなステップ数とOOFに対する予測値を取ることが目的\n",
    "# for fold, (train_loader, valid_loader) in enumerate(dataloaders):\n",
    "#     logger.info(f\"\\n FOLD{fold} : Training Start \\n\")\n",
    "\n",
    "#     # First Training\n",
    "#     trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "#     best_score, best_steps_, _ = trainer.train(train_loader, valid_loader)\n",
    "#     if config.smooth_type == \"online\":\n",
    "#         loss_soft_matrix = trainer.loss_fn.soft_matrix\n",
    "#     best_steps.append(best_steps_)\n",
    "#     logger.info(f\"\\n FOLD{fold} : First Training Done! -->> Best Score: {best_score}, Best Steps: {best_steps_} \\n\")\n",
    "\n",
    "#     del trainer\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # Create High-Quality Dataloader\n",
    "#     train_dataset = train_loader.dataset\n",
    "#     train_dataset.drop_first_only_data()\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         sampler=get_sampler(train_dataset),\n",
    "#         batch_size=config.train_batch,\n",
    "#         collate_fn=collate_fn,\n",
    "#         pin_memory=True,\n",
    "#         drop_last=True,\n",
    "#     )\n",
    "\n",
    "#     # Additional Training\n",
    "#     trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "#     if config.smooth_type == \"online\":\n",
    "#         trainer.loss_fn.soft_matrix = loss_soft_matrix.clone()\n",
    "#     best_score, best_add_steps_, oof_df = trainer.train(\n",
    "#         train_loader,\n",
    "#         valid_loader,\n",
    "#         retrain=True,\n",
    "#         retrain_weight_name=f\"model_fold{fold}_best\",\n",
    "#         retrain_best_score=best_score,\n",
    "#     )\n",
    "#     best_add_steps.append(best_add_steps_)\n",
    "#     oof_df.write_parquet(config.output_path / f\"oof_fold{fold}.parquet\")\n",
    "#     oof_dfs.append(oof_df)\n",
    "#     logger.info(\n",
    "#         f\"\\n FOLD{fold} : Additional Training Done! -->> Best Score: {best_score}, Best Add Steps: {best_add_steps_} \\n\"\n",
    "#     )\n",
    "\n",
    "#     del train_loader, valid_loader, train_dataset, trainer, oof_df\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# del dataloaders\n",
    "# gc.collect()\n",
    "\n",
    "# # Save OOF\n",
    "# oof_df = pl.concat(oof_dfs)\n",
    "# oof_df.write_parquet(config.output_path / \"oof.parquet\")\n",
    "# del oof_dfs\n",
    "# gc.collect()\n",
    "\n",
    "# # Get Best Negative Threshold\n",
    "# best_score, best_th = get_best_negative_threshold(config, oof_df)\n",
    "# message = f\"Overall OOF Best Score: {best_score}, Best Negative Threshold: {best_th}\"\n",
    "# logger.info(message)\n",
    "# config.negative_th = best_th.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# これなら通る\n",
    "# best_steps = [10, 5, 5]\n",
    "# best_add_steps = [10, 5, 5]\n",
    "\n",
    "# これだと落ちる -> メモリーエラーではない\n",
    "N = 100\n",
    "best_steps = [N, N, N]\n",
    "best_add_steps = [N, N, N]\n",
    "\n",
    "collate_fn = CollateFn(get_tokenizer(config), is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-19 16:30:49\u001b[0m | \u001b[1mINFO ] \n",
      " Full Train : Training Start \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f37197b774479c8f14976e24892299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce9bd771cac413c9527dec7bf5f2199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-19 16:31:24\u001b[0m | \u001b[1mINFO ] \n",
      " Full Train : First Training Done! \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # 全データ学習を行う\n",
    "if config.full_train:\n",
    "    full_steps = np.max(best_steps)\n",
    "    full_add_steps = np.max(best_add_steps)\n",
    "    logger.info(\"\\n Full Train : Training Start \\n\")\n",
    "    train_loader = get_full_train_loader(config, data)\n",
    "\n",
    "    # First Training\n",
    "    trainer = Trainer(config, logger, save_suffix=\"\")\n",
    "    trainer.train(train_loader, valid_loader=None, full_train=True, full_steps=full_steps)\n",
    "    if config.smooth_type == \"online\":\n",
    "        loss_soft_matrix = trainer.loss_fn.soft_matrix\n",
    "    logger.info(\"\\n Full Train : First Training Done! \\n\")\n",
    "\n",
    "    trainer.model.to(\"cpu\")\n",
    "    # trainer.model_ema.to(\"cpu\")\n",
    "    del trainer.model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Create High-Quality Dataloader\n",
    "    train_dataset = train_loader.dataset\n",
    "    train_dataset.drop_first_only_data()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=get_sampler(train_dataset),\n",
    "        batch_size=config.train_batch,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # Additional Training\n",
    "    trainer = Trainer(config, logger, save_suffix=\"\")\n",
    "    if config.smooth_type == \"online\":\n",
    "        trainer.loss_fn.soft_matrix = loss_soft_matrix.clone()\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        valid_loader=None,\n",
    "        retrain=True,\n",
    "        retrain_weight_name=\"model_full\",\n",
    "        full_train=True,\n",
    "        full_steps=full_add_steps,\n",
    "    )\n",
    "    logger.info(\"\\n Full Train : Additional Training Done! \\n\")\n",
    "\n",
    "    del train_loader, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Additional Training\n",
    "trainer = Trainer(config, logger, save_suffix=\"\")\n",
    "# if config.smooth_type == \"online\":\n",
    "#     trainer.loss_fn.soft_matrix = loss_soft_matrix.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.load import load_competition_data\n",
    "from src.data.utils import get_original_token_df, get_truth_df\n",
    "from src.utils.constant import IDX2TARGET_WITH_BIO, TARGET2IDX_WITH_BIO\n",
    "\n",
    "\n",
    "class PostProcessor:\n",
    "    \"\"\"\n",
    "    後処理を行うクラス\n",
    "\n",
    "    後処理一覧\n",
    "        - 1. 空白文字の予測を0(\"O\")に置き換える\n",
    "        - 2. Prefixの妥当性を確保する\n",
    "            - 2-1. B, Bが連続していて、かつ同一のタイプである場合に、後続のBをIに変更する\n",
    "            - 2-2. Iの前がOで、かつ閾値以内に同一タイプのB, Iが存在する場合に、間のOをIに変更する\n",
    "            - 2-3. Iの前がOで、かつ2-2の場合でない場合に、IをBに変更する\n",
    "        - 3. ラベルのタイプが混在している場合に、それを1つのラベルに統合して、さらにFPを弾く処理を行う\n",
    "\n",
    "    Expected DataFrame\n",
    "        - pred_df (pl.DataFrame): [document, token_idx, prob, pred]\n",
    "            - 全てのトークンに対しての予測を含める必要がある(\"O\"の場合も除去しない)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DictConfig):\n",
    "        self.config = config\n",
    "        self.org_data = load_json_data(config.input_path / f\"{config.run_type}.json\", debug=config.debug)\n",
    "        self.full_texts = [d[\"full_text\"] for d in self.org_data]\n",
    "\n",
    "    def post_process(self, pred_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        pred_df = pred_df.with_columns(\n",
    "            pred_org=pl.col(\"pred\").replace(IDX2TARGET_WITH_BIO, default=\"\"),\n",
    "        )\n",
    "        pred_doc_ids = pred_df[\"document\"].unique().to_numpy()\n",
    "\n",
    "        org_token_df = get_original_token_df(self.config, pred_doc_ids)\n",
    "        pred_df = pred_df.join(org_token_df, on=[\"document\", \"token_idx\"], how=\"left\", coalesce=True)\n",
    "\n",
    "        if self.config.run_type == \"train\":\n",
    "            truth_df = get_truth_df(config, pred_doc_ids, convert_idx=True)\n",
    "            truth_df = truth_df.with_columns(label_org=pl.col(\"label\").repclace(IDX2TARGET_WITH_BIO, default=\"\"))\n",
    "            pred_df = pred_df.join(truth_df, on=[\"document\", \"token_idx\"], how=\"left\", coalesce=True)\n",
    "\n",
    "        pred_df = pred_df.with_columns(pred_org_prev=pl.col(\"pred_org\"))\n",
    "\n",
    "        # 1. 空白文字の予測を0(\"O\")に置き換える\n",
    "        pred_df = self.remove_space_pii(pred_df)\n",
    "\n",
    "        # 2. Prefixの妥当性を確保する\n",
    "        pred_df = self.check_prefix_validity(pred_df, rule1=True, rule2=False, rule3=True)\n",
    "        # 2の処理で空白に再度PIIが予測される場合があるため、再度1の処理を行う\n",
    "        pred_df = self.remove_space_pii(pred_df)\n",
    "\n",
    "        # 3. ラベルのタイプが混在している場合に、それを1つのラベルに統合して、さらにFPを弾く処理を行う\n",
    "        pred_df = self.check_label_validity(pred_df)\n",
    "\n",
    "        return pred_df\n",
    "\n",
    "    def remove_space_pii(self, pred_df: pl.DataFrame) -> pl.DataFrame:\n",
    "        pred_df = pred_df.with_columns(\n",
    "            pred=(\n",
    "                pl.when(pl.col(\"token\").map_elements(lambda x: re.sub(r\"[ \\xa0]+\", \" \", x)) == \" \")\n",
    "                .then(pl.lit(0))\n",
    "                .otherwise(pl.col(\"pred\"))\n",
    "            ),\n",
    "            pred_org=(\n",
    "                pl.when(pl.col(\"token\").map_elements(lambda x: re.sub(r\"[ \\xa0]+\", \" \", x)) == \" \")\n",
    "                .then(pl.lit(\"O\"))\n",
    "                .otherwise(pl.col(\"pred_org\"))\n",
    "            ),\n",
    "        )\n",
    "        return pred_df\n",
    "\n",
    "    def check_prefix_validity(\n",
    "        self,\n",
    "        pred_df: pl.DataFrame,\n",
    "        rule1: bool = True,\n",
    "        rule2: bool = True,\n",
    "        rule3: bool = True,\n",
    "        rule2_th: int = 2,\n",
    "    ) -> pl.DataFrame:\n",
    "        pred_df = pred_df.sort([\"document\", \"token_idx\"])\n",
    "\n",
    "        pred_df_ = []\n",
    "        for _, doc_df in tqdm(pred_df.group_by(\"document\"), total=pred_df[\"document\"].n_unique()):\n",
    "            preds_org = doc_df[\"pred_org\"].to_numpy()\n",
    "            new_preds_org = preds_org.copy()\n",
    "\n",
    "            for i in tqdm(range(len(doc_df)), desc=\"Check Prefix Validity\"):\n",
    "                pred_org = preds_org[i]\n",
    "                if pred_org == \"O\":\n",
    "                    continue\n",
    "\n",
    "                prefix, label_type = pred_org.split(\"-\")\n",
    "                prefix_p1, label_type_p1 = self.get_prev_pred_label(new_preds_org, i - 1)\n",
    "\n",
    "                # 1. B, Bが連続していて、かつ同一のタイプである場合に、後続のBをIに変更する\n",
    "                if rule1:\n",
    "                    if prefix == \"B\" and prefix_p1 == \"B\" and label_type == label_type_p1:\n",
    "                        new_preds_org[i] = f\"I-{label_type}\"\n",
    "\n",
    "                if prefix == \"I\" and prefix_p1 == \"O\":\n",
    "                    finding = False\n",
    "\n",
    "                    # 2. Iの前がOで、かつ閾値以内に同一タイプのB, Iが存在する場合に、間のOをIに変更する\n",
    "                    if rule2:\n",
    "                        for j in range(2, rule2_th + 1):\n",
    "                            prefix_p, label_type_p = self.get_prev_pred_label(new_preds_org, i - j)\n",
    "                            if prefix_p in [\"B\", \"I\"] and label_type == label_type_p:\n",
    "                                finding = True\n",
    "                                for k in range(1, j):\n",
    "                                    new_preds_org[i - k] = f\"I-{label_type}\"\n",
    "\n",
    "                    # 3. Iの前がOで、かつ2の場合でない場合に、IをBに変更する\n",
    "                    if rule3:\n",
    "                        if finding == False:\n",
    "                            new_preds_org[i] = f\"B-{label_type}\"\n",
    "\n",
    "            doc_df = doc_df.with_columns(pred_org=pl.Series(new_preds_org.tolist()).cast(pl.Utf8)).with_columns(\n",
    "                pred=pl.col(\"pred_org\").replace(TARGET2IDX_WITH_BIO, default=0)\n",
    "            )\n",
    "            pred_df_.append(doc_df)\n",
    "\n",
    "        pred_df = pl.concat(pred_df_)\n",
    "        return pred_df\n",
    "\n",
    "    def check_label_validity(self, pred_df: pl.DataFrame):\n",
    "        # 各PIIに固有のグループIDを割り当てる\n",
    "        pred_df = pred_df.with_row_index(name=\"group_idx\").with_columns(\n",
    "            prefix=pl.col(\"pred_org\").map_elements(lambda x: x if x == \"O\" else x.split(\"-\")[0])\n",
    "        )\n",
    "        pred_df = pred_df.with_columns(\n",
    "            group_idx=(\n",
    "                pl.when(pl.col(\"prefix\") == \"I\")\n",
    "                .then(pl.lit(None))\n",
    "                .when(pl.col(\"prefix\") == \"O\")\n",
    "                .then(pl.lit(-1))\n",
    "                .otherwise(pl.col(\"group_idx\"))\n",
    "            )\n",
    "        )\n",
    "        pred_df = pred_df.sort([\"document\", \"token_idx\"])\n",
    "        pred_df = pred_df.with_columns(group_idx=pl.col(\"group_idx\").fill_null(strategy=\"forward\").over(\"document\"))\n",
    "\n",
    "        # 一度pandasに変換しないと激遅 (原因不明, メモリの配置やソートの問題ではないっぽい)\n",
    "        pred_df = pred_df.to_pandas()\n",
    "        pred_df = pl.from_pandas(pred_df)\n",
    "\n",
    "        pred_df_ = []\n",
    "        for (_, group_idx), group_df in tqdm(\n",
    "            pred_df.group_by([\"document\", \"group_idx\"]),\n",
    "            total=len(pred_df.unique(subset=[\"document\", \"group_idx\"])),\n",
    "            desc=\"Check PII Validity\",\n",
    "        ):\n",
    "            if group_idx == -1:\n",
    "                pred_df_.append(group_df)\n",
    "                continue\n",
    "\n",
    "            # Bから始まり、それ以外はIであることを確認\n",
    "            for i, prefix in enumerate(group_df[\"prefix\"].to_list()):\n",
    "                if i == 0:\n",
    "                    assert prefix == \"B\"\n",
    "                else:\n",
    "                    assert prefix == \"I\"\n",
    "\n",
    "            group_df = group_df.with_columns(\n",
    "                label_type=pl.col(\"pred_org\").map_elements(lambda x: x.split(\"-\")[1] if x != \"O\" else None)\n",
    "            )\n",
    "\n",
    "            # ラベルタイプが異なる場合は、最も確率が高いタイプに統一する\n",
    "            if group_df[\"label_type\"].n_unique() > 1:\n",
    "                highest_type = (\n",
    "                    group_df.group_by(\"label_type\")\n",
    "                    .agg(pl.col(\"prob\").sum())\n",
    "                    .sort(\"prob\", descending=True)[\"label_type\"][0]\n",
    "                )\n",
    "                # 変更される行の確率は元のままであることに注意 -> Ensembleの手法はVotingだからあまり関係はない\n",
    "                group_df = group_df.with_columns(\n",
    "                    pred_org=pl.concat_str([pl.col(\"prefix\"), pl.lit(\"-\"), pl.lit(highest_type)])\n",
    "                )\n",
    "                group_df = group_df.with_columns(pred=pl.col(\"pred_org\").replace(TARGET2IDX_WITH_BIO, default=0))\n",
    "\n",
    "            # FPを弾く処理を行う\n",
    "            group_df = self.remove_false_positive(group_df)\n",
    "            pred_df_.append(group_df)\n",
    "\n",
    "        # 全体を結合する\n",
    "        pred_df_ = (\n",
    "            pl.concat(pred_df_, how=\"diagonal\")\n",
    "            .sort(\"document\", \"token_idx\")\n",
    "            .drop([\"group_idx\", \"prefix\", \"label_type\"])\n",
    "        )\n",
    "        return pred_df_\n",
    "\n",
    "    def get_prev_pred_label(self, preds_array: np.ndarray, idx: int):\n",
    "        if idx >= 0:\n",
    "            label = preds_array[idx]\n",
    "            if label == \"O\":\n",
    "                return \"O\", None\n",
    "            else:\n",
    "                return label.split(\"-\")\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def remove_false_positive(\n",
    "        self,\n",
    "        pii_df: pl.DataFrame,\n",
    "        name_student: bool = True,\n",
    "        email: bool = True,\n",
    "        username: bool = True,\n",
    "        id_num: bool = True,\n",
    "        phone_num: bool = False,\n",
    "        url_personal: bool = False,\n",
    "        street_address: bool = False,\n",
    "        unq_count_rule: bool = False,\n",
    "    ):\n",
    "        label_type = pii_df[\"label_type\"][0]\n",
    "        pii_string = self.get_pii_string(pii_df)\n",
    "\n",
    "        if name_student and label_type == \"NAME_STUDENT\":\n",
    "            # トークンの先頭の文字が大文字でない場合かつ,1文字の場合に除去\n",
    "            pii_df = pii_df.with_columns(\n",
    "                pred=pl.when(pl.col(\"token\").map_elements(lambda x: (x[0].isupper()) and (len(x.strip()) > 1)))\n",
    "                .then(pl.col(\"pred\"))\n",
    "                .otherwise(pl.lit(0)),\n",
    "                pred_org=pl.when(pl.col(\"token\").map_elements(lambda x: (x[0].isupper()) and (len(x.strip()) > 1)))\n",
    "                .then(pl.col(\"pred_org\"))\n",
    "                .otherwise(pl.lit(\"O\")),\n",
    "            )\n",
    "\n",
    "        elif email and label_type == \"EMAIL\":\n",
    "            # 下記のフォーマットでマッチしない場合は除去する\n",
    "            email_pattern = r\"[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.(?:com|org|edu)\"  # [任意の文字列]@[任意の文字列].com|org|edu\n",
    "            if re.search(email_pattern, pii_string) is None:\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "                return pii_df\n",
    "\n",
    "        elif username and label_type == \"USERNAME\":\n",
    "            # トークンが1文字の場合に除去\n",
    "            pii_df = self.remove_pii_based_short_token(pii_df, len_th=1)\n",
    "\n",
    "        elif id_num and label_type == \"ID_NUM\":\n",
    "            # トークンが1文字の場合に除去\n",
    "            pii_df = self.remove_pii_based_short_token(pii_df, len_th=1)\n",
    "\n",
    "            # N桁未満の数字のみの場合に除去\n",
    "            num_only = re.search(r\"^\\d+$\", pii_string)\n",
    "            if num_only is not None and len(pii_string) < 4:\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "                return pii_df\n",
    "\n",
    "        elif phone_num and label_type == \"PHONE_NUM\":\n",
    "            # 数字以外を除去して数字がN桁(電話番号の最小桁数は10桁)未満の場合は除去する\n",
    "            num_string = re.sub(r\"\\D+\", \"\", pii_string)\n",
    "            if len(num_string) < 10:\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "                return pii_df\n",
    "\n",
    "        elif url_personal and label_type == \"URL_PERSONAL\":\n",
    "            # 下記のフォーマットでマッチしない場合は除去する\n",
    "            url_pattern = r\"^(?:http|https)://\"  # http://, https://で始まるURL\n",
    "            if re.search(url_pattern, pii_string) is None:\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "                return pii_df\n",
    "\n",
    "            # 以下を含む場合は除去する\n",
    "            remove_target = [\n",
    "                \"wikipedia.org\",\n",
    "            ]\n",
    "            if any([target in pii_string for target in remove_target]):\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "                return pii_df\n",
    "\n",
    "        elif street_address and label_type == \"STREET_ADDRESS\":\n",
    "            # 10文字未満であれば除去する\n",
    "            if len(pii_string) < 10:\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "                return pii_df\n",
    "\n",
    "        # pii_stringが出現するテキストのユニークな数をカウントする -> 一定以上の生徒のテキストで出現する場合は除去する\n",
    "        if unq_count_rule:\n",
    "            unq_appear_count = len([text for text in self.full_texts if pii_string in text])\n",
    "            condition = ((unq_appear_count >= 3) and (label_type != \"NAME_STUDENT\")) or (\n",
    "                (unq_appear_count >= 10) and (label_type == \"NAME_STUDENT\")\n",
    "            )\n",
    "            if condition:\n",
    "                pii_df = self.remove_pii(pii_df)\n",
    "\n",
    "        return pii_df\n",
    "\n",
    "    def get_pii_string(self, pii_df: pl.DataFrame):\n",
    "        pii_string = \"\"\n",
    "        for token, space in pii_df[[\"token\", \"space\"]].to_numpy():\n",
    "            if space:\n",
    "                pii_string += token + \" \"\n",
    "            else:\n",
    "                pii_string += token\n",
    "        return pii_string.strip()\n",
    "\n",
    "    def remove_pii(self, pii_df: pl.DataFrame):\n",
    "        pii_df = pii_df.with_columns(pred=pl.lit(0).cast(pl.Int64), pred_org=pl.lit(\"O\"))\n",
    "        return pii_df\n",
    "\n",
    "    def remove_pii_based_short_token(self, pii_df: pl.DataFrame, len_th: int):\n",
    "        pii_df = pii_df.with_columns(\n",
    "            pred=pl.when(pl.col(\"token\").map_elements(lambda x: len(x.strip()) <= len_th))\n",
    "            .then(pl.lit(0))\n",
    "            .otherwise(pl.col(\"pred\")),  # char_len以下の文字数のトークンを除去\n",
    "            pred_org=pl.when(pl.col(\"token\").map_elements(lambda x: len(x.strip()) <= len_th))\n",
    "            .then(pl.lit(\"O\"))\n",
    "            .otherwise(pl.col(\"pred_org\")),\n",
    "        )\n",
    "        return pii_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後処理で精度がどれくらい変わるのかを確認\n",
    "oof_df = pl.read_parquet(config.oof_path / f\"oof_{config.exp}{suffix}.parquet\")\n",
    "pred_df = get_pred_df(oof_df, negative_th=config.negative_th)\n",
    "\n",
    "pper = PostProcessor(config, pred_df)\n",
    "pred_df = pper.get_post_processed_df()\n",
    "\n",
    "score = evaluate_metric(\n",
    "    pred_df,\n",
    "    get_truth_df(config, pred_df[\"document\"].unique().to_list(), is_label_idx=True),\n",
    "    hoge=\"hogehogehogehogehogehogehogehoge\",\n",
    ")\n",
    "print(f\"Post Processed Score: {score:.4f}\")\n",
    "logger.info(f\"Post Processed Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"lasdkf\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
