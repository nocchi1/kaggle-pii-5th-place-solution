{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.preprocess import DataProvider1st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = \"087\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-10 08:19:04\u001b[0m | \u001b[1mINFO ] exp:087 start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path(\"../config\"))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f\"exp:{exp} start\")\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nicholas', True], ['mpware', False], ['pjma', False]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.debug = True\n",
    "config.exter_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムな位置に出現するようにするギミック\n",
    "# ラベルスムージングをどこで入れるか\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "import random\n",
    "from typing import List, Literal\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import statistics\n",
    "import itertools\n",
    "\n",
    "\n",
    "from src.utils.competition_utils import (\n",
    "    mapping_index_org2char,\n",
    "    mapping_index_char2token,\n",
    "    mapping_index_char2token_overlapped,\n",
    ")\n",
    "\n",
    "\n",
    "class DetectDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DictConfig,\n",
    "        data: List[dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        data_type: Literal[\"train\", \"valid\", \"test\"],\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.data_type = data_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_ids = [d[\"document\"] for d in data]\n",
    "        self.full_texts = [d[\"full_text\"] for d in data]\n",
    "        self.org_tokens = [d[\"tokens\"] for d in data]\n",
    "        self.whitespaces = [d[\"trailing_whitespace\"] for d in data]\n",
    "        self.additionals = [d[\"additional\"] for d in data]\n",
    "\n",
    "        stride = config.train_stride if data_type == \"train\" else config.eval_stride\n",
    "        tokens = tokenizer(\n",
    "            self.full_texts,\n",
    "            max_length=config.max_length,\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "        self.offset_mapping = tokens[\"offset_mapping\"]\n",
    "        self.overflow_mapping = tokens[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "        # 外部データはidxがstr型なので全てstr型に変換されることに注意\n",
    "        self.overlap_doc_ids = np.array(self.doc_ids)[self.overflow_mapping]\n",
    "        self.overlap_additional = np.array(self.additionals)[self.overflow_mapping]\n",
    "\n",
    "        org_tokens_idx = [list(range(len(d[\"tokens\"]))) for d in data]\n",
    "        org_tokens_len = {str(d[\"document\"]): len(d[\"tokens\"]) for d in data}\n",
    "\n",
    "        char_org_idx = mapping_index_org2char(\n",
    "            self.full_texts,\n",
    "            self.org_tokens,\n",
    "            org_tokens_idx,\n",
    "            self.whitespaces,\n",
    "            fill_val=-1,\n",
    "        )\n",
    "        token_org_idx = mapping_index_char2token_overlapped(\n",
    "            char_org_idx,\n",
    "            self.input_ids,\n",
    "            self.offset_mapping,\n",
    "            self.overflow_mapping,\n",
    "            fill_val=-1,\n",
    "        )\n",
    "\n",
    "        # 相対的な位置情報\n",
    "        self.positions_ratio = [\n",
    "            np.clip(np.array(org_idx) / org_tokens_len[str(doc_id)], 0, None)\n",
    "            for org_idx, doc_id in zip(token_org_idx, self.overlap_doc_ids)\n",
    "        ]\n",
    "        # 絶対的な位置情報\n",
    "        self.positions_abs = [\n",
    "            np.clip(np.array(org_idx) / 3298, 0, None) for org_idx in token_org_idx\n",
    "        ]  # 3298 is the maximum token length of dataset\n",
    "\n",
    "        # ラベルを取得する\n",
    "        if data_type in [\"train\", \"valid\"]:\n",
    "            self.org_labels = [d[\"labels\"] for d in data]\n",
    "            org_labels_dict = {str(d[\"document\"]): np.array(d[\"labels\"]) for d in data}\n",
    "\n",
    "            self.token_labels = []\n",
    "            for org_idx, doc_id in zip(token_org_idx, self.overlap_doc_ids):\n",
    "                label = org_labels_dict[str(doc_id)][org_idx]\n",
    "                space_idx = np.where(np.array(org_idx) == -1)[0]\n",
    "                label[space_idx] = -1  # -1になっている場合は-1を保持する必要がある\n",
    "                self.token_labels.append(label)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self.data_type in [\"train\", \"valid\"]:\n",
    "            return (\n",
    "                torch.tensor(self.input_ids[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                torch.tensor(self.attention_mask[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                torch.tensor(self.positions_ratio[idx], dtype=torch.float, device=\"cpu\"),\n",
    "                torch.tensor(self.positions_abs[idx], dtype=torch.float, device=\"cpu\"),\n",
    "                torch.tensor(self.token_labels[idx], dtype=torch.long, device=\"cpu\"),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.tensor(self.input_ids[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                torch.tensor(self.attention_mask[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                torch.tensor(self.positions_ratio[idx], dtype=torch.float, device=\"cpu\"),\n",
    "                torch.tensor(self.positions_abs[idx], dtype=torch.float, device=\"cpu\"),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def drop_first_only_data(self):\n",
    "        \"\"\"\n",
    "        追加学習を行う際に, 1段階目のみ使用するデータセットを削除する\n",
    "        \"\"\"\n",
    "        assert self.data_type == \"train\"\n",
    "        mask = self.overlap_additional\n",
    "        self.input_ids = list(itertools.compress(self.input_ids, mask))\n",
    "        self.attention_mask = list(itertools.compress(self.attention_mask, mask))\n",
    "        self.positions_ratio = list(itertools.compress(self.positions_ratio, mask))\n",
    "        self.positions_abs = list(itertools.compress(self.positions_abs, mask))\n",
    "        self.token_labels = list(itertools.compress(self.token_labels, mask))\n",
    "\n",
    "\n",
    "class DetectRandomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DictConfig,\n",
    "        data: List[dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        data_type: Literal[\"train\", \"valid\", \"test\"],\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.data_type = data_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_ids = [d[\"document\"] for d in data]\n",
    "        self.full_texts = [d[\"full_text\"] for d in data]\n",
    "        self.org_tokens = [d[\"tokens\"] for d in data]\n",
    "        self.whitespaces = [d[\"trailing_whitespace\"] for d in data]\n",
    "        self.additionals = [d[\"additional\"] for d in data]\n",
    "\n",
    "        tokens = tokenizer(self.full_texts, return_offsets_mapping=True)\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "        self.offset_mapping = tokens[\"offset_mapping\"]\n",
    "        self.overflow_mapping = tokens[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpr = DataProvider1st(config, \"train\")\n",
    "data = dpr.load_data()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-the-learning-agency-lab-pii-data-detection/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DetectDataset(config, data, tokenizer, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = dataset.input_ids\n",
    "additionals = dataset.additionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random位置にpiiが含まれるようにwindowを作成する -> cv下がったのでこれは使用しない\n",
    "class FirstStageDatasetWithRandom(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        data: List[dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        data_type: Literal[\"train\", \"valid\", \"test\"],\n",
    "    ):\n",
    "        if get_window == \"random\":\n",
    "            assert data_type == \"train\"  # randomはtrainのみのオプション\n",
    "\n",
    "        self.config = config\n",
    "        self.get_window = get_window\n",
    "        self.data_type = data_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_ids = [d[\"document\"] for d in data]\n",
    "        self.texts = [d[\"full_text\"] for d in data]\n",
    "        self.org_tokens = [d[\"tokens\"] for d in data]\n",
    "        self.whitespace = [d[\"trailing_whitespace\"] for d in data]\n",
    "\n",
    "        if get_window == \"overlap\":\n",
    "            stride = config.train_stride if data_type == \"train\" else config.eval_stride\n",
    "            tokens = tokenizer(\n",
    "                self.texts,\n",
    "                max_length=config.max_length,\n",
    "                stride=stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "        elif get_window == \"random\":\n",
    "            tokens = tokenizer(\n",
    "                self.texts,\n",
    "                return_offsets_mapping=True,\n",
    "            )\n",
    "\n",
    "        self.input_ids = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "        self.offset_mapping = tokens[\"offset_mapping\"]\n",
    "        if get_window == \"overlap\":\n",
    "            self.overflow_mapping = tokens[\"overflow_to_sample_mapping\"]\n",
    "            self.overlap_doc_ids = np.array(self.doc_ids)[self.overflow_mapping]\n",
    "\n",
    "        if self.data_type in [\"train\", \"valid\"]:\n",
    "            self.org_labels = [d[\"labels\"] for d in data]\n",
    "            self.char_labels = convert_org2char_labels(self.texts, self.org_tokens, self.whitespace, self.org_labels)\n",
    "            if get_window == \"overlap\":\n",
    "                self.token_labels = convert_char2token_labels_overlapped(\n",
    "                    self.char_labels, self.input_ids, self.offset_mapping, self.overflow_mapping\n",
    "                )\n",
    "            elif get_window == \"random\":\n",
    "                self.token_labels = convert_char2token_labels(\n",
    "                    self.char_labels, self.input_ids, self.offset_mapping, fill_val=0\n",
    "                )\n",
    "                self.pii_locs = self.find_pii_locations(self.doc_ids, self.token_labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # doc_ids, input_ids, attention_mask, token_labels, offset_mapping\n",
    "        if self.get_window == \"overlap\":\n",
    "            if self.data_type in [\"train\", \"valid\"]:\n",
    "                return (\n",
    "                    torch.tensor(self.input_ids[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                    torch.tensor(self.attention_mask[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                    torch.tensor(self.token_labels[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                )\n",
    "            else:\n",
    "                return (\n",
    "                    torch.tensor(self.input_ids[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                    torch.tensor(self.attention_mask[idx], dtype=torch.long, device=\"cpu\"),\n",
    "                )\n",
    "        elif self.get_window == \"random\":  # randomはtrainのみ\n",
    "            if (\n",
    "                random.random() >= 0.50\n",
    "            ):  # 50%の確率でnegative sampling(ランダムなwindowを返す -> ほとんどnegativeになる)\n",
    "                global_idx, _, token_idx = self.pii_locs[idx]\n",
    "                input_ids = self.input_ids[global_idx]\n",
    "                attention_mask = self.attention_mask[global_idx]\n",
    "                token_labels = self.token_labels[global_idx]\n",
    "            else:\n",
    "                global_idx = random.choice(range(len(self.input_ids)))\n",
    "                input_ids = self.input_ids[global_idx]\n",
    "                attention_mask = self.attention_mask[global_idx]\n",
    "                token_labels = self.token_labels[global_idx]\n",
    "                token_idx = random.choice(range(len(input_ids)))\n",
    "\n",
    "            left_length = random.randint(0, self.config.max_length - 1)\n",
    "            start_idx = max(0, token_idx - left_length)\n",
    "            end_idx = start_idx + self.config.max_length\n",
    "            excess_right_length = max(0, end_idx - len(input_ids))\n",
    "            start_idx = max(0, start_idx - excess_right_length)\n",
    "            end_idx = end_idx - excess_right_length\n",
    "            assert start_idx <= token_idx and token_idx <= end_idx\n",
    "\n",
    "            input_ids = input_ids[start_idx:end_idx]\n",
    "            attention_mask = attention_mask[start_idx:end_idx]\n",
    "            token_labels = token_labels[start_idx:end_idx]\n",
    "\n",
    "            return (\n",
    "                torch.tensor(input_ids, dtype=torch.long, device=\"cpu\"),\n",
    "                torch.tensor(attention_mask, dtype=torch.long, device=\"cpu\"),\n",
    "                torch.tensor(token_labels, dtype=torch.long, device=\"cpu\"),\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.get_window == \"overlap\":\n",
    "            return len(self.input_ids)\n",
    "        elif self.get_window == \"random\":\n",
    "            return len(self.pii_locs)\n",
    "\n",
    "    def find_pii_locations(self, doc_ids: List[int], token_labels: List[int]) -> List[List[int]]:\n",
    "        # 各documentのtoken_labelsからPIIの位置を特定する\n",
    "        pii_locs = []\n",
    "        for i, (doc_id, token_label) in enumerate(zip(doc_ids, token_labels)):\n",
    "            is_pii_array = np.where(token_label != 0, 1, 0)\n",
    "            is_pii_diff = np.where(np.diff(is_pii_array, prepend=0) == 1, 1, 0)\n",
    "            pii_index = np.where(is_pii_diff == 1)[0]\n",
    "            for idx in pii_index:\n",
    "                pii_locs.append([i, doc_id, idx])\n",
    "        return pii_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.torch.first_stage.dataset import FirstStageDataset\n",
    "from src.torch.first_stage.collate_fn import CollateFn\n",
    "from src.torch.data_utils import get_sampler\n",
    "\n",
    "\n",
    "def get_train_loaders(\n",
    "    config,\n",
    "    data: List[dict],\n",
    "    fold_array: np.ndarray = None,\n",
    ") -> List[Tuple[DataLoader, DataLoader]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        use_fold: if not None, only use the first `use_fold` folds\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "    if config.add_newline_token:\n",
    "        tokenizer.add_tokens([\"\\n\", \"\\r\"], special_tokens=True)\n",
    "    collate_fn = CollateFn(tokenizer, is_train=True)\n",
    "\n",
    "    dataloaders = []\n",
    "    for fold in range(config.n_fold):\n",
    "        if config.use_fold is not None and config.use_fold <= fold:\n",
    "            break\n",
    "        train_idx = np.where(fold_array != fold)[0]\n",
    "        valid_idx = np.where(fold_array == fold)[0]\n",
    "        train_data = data[train_idx]\n",
    "        valid_data = data[valid_idx]\n",
    "\n",
    "        train_dataset = FirstStageDataset(\n",
    "            config,\n",
    "            train_data,\n",
    "            tokenizer,\n",
    "            data_type=\"train\",\n",
    "        )\n",
    "        valid_dataset = FirstStageDataset(config, valid_data, tokenizer, data_type=\"valid\")\n",
    "        train_sampler = get_sampler(train_dataset)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=config.train_batch,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=config.eval_batch,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        dataloaders.append((train_loader, valid_loader))\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def get_full_train_loader(config, data: List[dict]) -> DataLoader:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "    if config.add_newline_token:\n",
    "        tokenizer.add_tokens([\"\\n\", \"\\r\"], special_tokens=True)\n",
    "\n",
    "    collate_fn = CollateFn(tokenizer, is_train=True)\n",
    "    train_dataset = FirstStageDataset(\n",
    "        config,\n",
    "        data,\n",
    "        tokenizer,\n",
    "        data_type=\"train\",\n",
    "    )\n",
    "    train_sampler = get_sampler(train_dataset)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=config.train_batch,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, tokenizer: AutoTokenizer, is_train: bool = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.is_train:\n",
    "            input_ids, attention_mask, token_labels = zip(*batch)\n",
    "        else:\n",
    "            input_ids, attention_mask = zip(*batch)\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        if self.is_train:\n",
    "            token_labels = pad_sequence(token_labels, batch_first=True, padding_value=-1)\n",
    "            return input_ids, attention_mask, token_labels\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "class FirstStageModel(nn.Module):\n",
    "    def __init__(self, config, class_num: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.use_hidden_states = config.use_hidden_states  # backboneのhidden_statesを何層使用するか\n",
    "        self.model_config = AutoConfig.from_pretrained(config.model_path)\n",
    "        self.model_config.update(\n",
    "            {\n",
    "                \"hidden_dropout_prob\": config.hidden_dropout,\n",
    "                \"attention_probs_dropout_prob\": config.attention_dropout,\n",
    "                \"output_hidden_states\": True,\n",
    "            }\n",
    "        )\n",
    "        hidden_size = self.model_config.hidden_size\n",
    "        self.backbone = AutoModel.from_pretrained(config.model_path, config=self.model_config)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.use_hidden_states, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(128, class_num),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self._init_weights(self.head)\n",
    "\n",
    "    # DeBERTa初期化関数\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = torch.cat(outputs.hidden_states[-self.use_hidden_states :], dim=-1)  # N層分のhidden_statesをconcat\n",
    "        x = self.dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def reinit_layers(self, reinit_layer_num: int):\n",
    "        \"\"\"\n",
    "        学習済みのweightを初期化して学習を行う\n",
    "        \"\"\"\n",
    "        for i in range(1, reinit_layer_num + 1):\n",
    "            self.backbone.encoder.layer[-i].apply(self._init_weights)\n",
    "\n",
    "    def freeze_layers(self, freeze_layer_num: int):\n",
    "        \"\"\"\n",
    "        N層目までをfreezeする\n",
    "        \"\"\"\n",
    "        for i in range(freeze_layer_num):\n",
    "            if i == 0:\n",
    "                for params in self.backbone.embeddings.parameters():\n",
    "                    params.requires_grad = False\n",
    "            else:\n",
    "                for params in self.backbone.encoder.layer[i - 1].parameters():\n",
    "                    params.requires_grad = False\n",
    "\n",
    "    def freeze_backbone(self, reinit_layer_num: int):\n",
    "        \"\"\"\n",
    "        N-step目まではbackboneをfreezeしてheadのみ学習を行う\n",
    "        \"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # reinit_layerに関してはfreezeしない\n",
    "        for i in range(reinit_layer_num):\n",
    "            for params in self.backbone.encoder.layer[i - 1].parameters():\n",
    "                params.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self, freeze_layer_num: int):\n",
    "        \"\"\"\n",
    "        N-step目以降はbackboneも学習を行う\n",
    "        \"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # freeze_layer_num層目までは学習全体を通してfreezeしておく\n",
    "        for i in range(freeze_layer_num):\n",
    "            if i == 0:\n",
    "                for params in self.backbone.embeddings.parameters():\n",
    "                    params.requires_grad = False\n",
    "            else:\n",
    "                for params in self.backbone.encoder.layer[i - 1].parameters():\n",
    "                    params.requires_grad = False\n",
    "\n",
    "\n",
    "class FirstStageModelWithGRU(nn.Module):\n",
    "    def __init__(self, config, class_num: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.use_hidden_states = config.use_hidden_states  # backboneのhidden_statesを何層使用するか\n",
    "        self.model_config = AutoConfig.from_pretrained(config.model_path)\n",
    "        self.model_config.update(\n",
    "            {\n",
    "                \"hidden_dropout_prob\": config.hidden_dropout,\n",
    "                \"attention_probs_dropout_prob\": config.attention_dropout,\n",
    "                \"output_hidden_states\": True,\n",
    "            }\n",
    "        )\n",
    "        hidden_size = self.model_config.hidden_size\n",
    "        self.backbone = AutoModel.from_pretrained(config.model_path, config=self.model_config)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.use_hidden_states, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(128, class_num),\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self._init_weights(self.linear)\n",
    "        self._init_weights(self.head)\n",
    "        # self._gru_init_weights(self.gru)\n",
    "\n",
    "    # DeBERTa初期化関数\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    # Tensorflow/Keras-like initialization for GRU\n",
    "    def _gru_init_weights(self, module):\n",
    "        for name, p in module.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif \"bias\" in name:\n",
    "                p.data.fill_(0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = torch.cat(outputs.hidden_states[-self.use_hidden_states :], dim=-1)  # N層分のhidden_statesをconcat\n",
    "        x = self.linear(x)\n",
    "        y, _ = self.gru(x)\n",
    "        y = self.layernorm1(y)\n",
    "        y = self.dropout(y)\n",
    "        x = torch.cat([x, y], dim=-1)\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def reinit_layers(self, reinit_layer_num: int):\n",
    "        \"\"\"\n",
    "        学習済みのweightを初期化して学習を行う\n",
    "        \"\"\"\n",
    "        for i in range(1, reinit_layer_num + 1):\n",
    "            self.backbone.encoder.layer[-i].apply(self._init_weights)\n",
    "\n",
    "    def freeze_layers(self, freeze_layer_num: int):\n",
    "        \"\"\"\n",
    "        N層目までをfreezeする\n",
    "        \"\"\"\n",
    "        for i in range(freeze_layer_num):\n",
    "            if i == 0:\n",
    "                for params in self.backbone.embeddings.parameters():\n",
    "                    params.requires_grad = False\n",
    "            else:\n",
    "                for params in self.backbone.encoder.layer[i - 1].parameters():\n",
    "                    params.requires_grad = False\n",
    "\n",
    "    def freeze_backbone(self, reinit_layer_num: int):\n",
    "        \"\"\"\n",
    "        N-step目まではbackboneをfreezeしてheadのみ学習を行う\n",
    "        \"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # reinit_layerに関してはfreezeしない\n",
    "        for i in range(reinit_layer_num):\n",
    "            for params in self.backbone.encoder.layer[i - 1].parameters():\n",
    "                params.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self, freeze_layer_num: int):\n",
    "        \"\"\"\n",
    "        N-step目以降はbackboneも学習を行う\n",
    "        \"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # freeze_layer_num層目までは学習全体を通してfreezeしておく\n",
    "        for i in range(freeze_layer_num):\n",
    "            if i == 0:\n",
    "                for params in self.backbone.embeddings.parameters():\n",
    "                    params.requires_grad = False\n",
    "            else:\n",
    "                for params in self.backbone.encoder.layer[i - 1].parameters():\n",
    "                    params.requires_grad = False\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    if config.with_gru:\n",
    "        model = FirstStageModelWithGRU(config, config.class_num)\n",
    "    else:\n",
    "        model = FirstStageModel(config, config.class_num)\n",
    "    model = model.to(config.device)\n",
    "    if config.reinit_layer_num > 0:\n",
    "        model.reinit_layers(config.reinit_layer_num)\n",
    "    if config.freeze_layer_num > 0:\n",
    "        model.freeze_layers(config.freeze_layer_num)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectModel(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ClassifyModel(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
