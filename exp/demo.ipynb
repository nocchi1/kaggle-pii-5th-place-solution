{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../config/exp_087_train.yaml\n",
    "exp: \"087\"\n",
    "run_type: \"train\"\n",
    "task_type: \"detect\"\n",
    "device: \"cuda\"\n",
    "seed: 10\n",
    "\n",
    "# data preprocess\n",
    "remove_prefix: true\n",
    "exter_dataset:\n",
    "  - [\"nicholas\", true]\n",
    "  - [\"mpware\", false]\n",
    "  - [\"pjma\", false]\n",
    "n_fold: 3\n",
    "use_fold: 3\n",
    "\n",
    "# dataset, dataloader\n",
    "add_newline_token: true\n",
    "max_length: 128\n",
    "train_stride: 96\n",
    "eval_stride: 64\n",
    "train_batch: 16\n",
    "eval_batch: 64\n",
    "\n",
    "# model\n",
    "model_path: \"microsoft/deberta-v3-large\"\n",
    "class_num: 8 # with prefix -> 13, without prefix -> 8\n",
    "lstm_type: \"none\"\n",
    "use_hidden_states: 2\n",
    "dropout: 0.10\n",
    "hidden_dropout: 0.10\n",
    "attention_dropout: 0.10\n",
    "reinit_layer_num: 0\n",
    "freeze_layer_num: 0\n",
    "\n",
    "# loss\n",
    "smooth_type: \"online\"\n",
    "smooth_ratio: 0.05\n",
    "smooth_pair: 0.05\n",
    "positive_class_weight: 10\n",
    "\n",
    "# optimizer\n",
    "optimizer_type: \"AdamW\"\n",
    "pretrained_lr: 1e-6\n",
    "head_lr: 1e-4\n",
    "weight_decay: 0.01\n",
    "betas: [0.9, 0.999]\n",
    "\n",
    "# scheduler\n",
    "scheduler_type: \"cosine_custom\"\n",
    "first_cycle_epochs: 4\n",
    "cycle_factor: 1\n",
    "num_warmup_steps: 0\n",
    "min_lr: 1e-9\n",
    "gamma: 1.0\n",
    "\n",
    "# training\n",
    "epochs: 4\n",
    "accumulation_steps: 2\n",
    "eval_steps: 1000\n",
    "negative_th: 0.660\n",
    "negative_th_method: \"overall\"\n",
    "amp: true\n",
    "ema: true\n",
    "ema_decay: 0.999\n",
    "ema_update_after_step: 8000\n",
    "\n",
    "# additional training\n",
    "add_train: true\n",
    "add_epochs: 4\n",
    "add_first_cycle_epochs: 4\n",
    "\n",
    "# full training\n",
    "full_train: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.postprocess import PostProcessor\n",
    "from src.preprocess import DetectDataReader\n",
    "from src.train import Trainer, get_full_train_loader, get_train_loaders\n",
    "from src.train.dataloader_utils import CollateFn, get_sampler, get_tokenizer\n",
    "from src.utils import TimeUtil, get_config, get_logger, seed_everything\n",
    "from src.utils.metric import get_best_negative_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]コマンドライン引数\n",
    "config_name = \"exp_082_train\"\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-22 01:41:56\u001b[0m | \u001b[1mINFO ] exp:082 start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(config_name, config_dir=Path(\"../config\"))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f\"exp:{config.exp} start\")\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n",
    "config.input_path = Path(\"../data/input\")\n",
    "config.exter_path = Path(\"../data/input/external\")\n",
    "config.output_path = Path(\"../data/output\") / config.exp\n",
    "config.output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config.debug = debug\n",
    "config.use_fold = 3\n",
    "config.eval_steps = 500  # 100\n",
    "config.ema_update_after_step = 100\n",
    "\n",
    "config.epochs = 2\n",
    "config.first_cycle_epochs = 2\n",
    "config.add_epochs = 2\n",
    "config.add_first_cycle_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-22 01:42:02\u001b[0m | \u001b[1mINFO ] Data Size: 13854\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dpr = DetectDataReader(config, \"train\")\n",
    "data = dpr.load_data()\n",
    "logger.info(f\"Data Size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.use_fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [TODO]データサイズを調整する\n",
    "\n",
    "# data_ = []\n",
    "# for fold in [-1, 0, 1, 2]:\n",
    "#     fold_data = [d for d in data if d[\"fold\"] == fold]\n",
    "#     fold_data = fold_data[:100]\n",
    "#     data_.extend(fold_data)\n",
    "\n",
    "# data = data_\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = get_train_loaders(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21110"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-21 08:05:41\u001b[0m | \u001b[1mINFO ] FOLD0 : Training Start...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b72cb1ecc004e6dae0b3f7a5d68c536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc93353a98344b682e6c8d0da8b1269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028ee87bea114e4387ad7c836eae05b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-21 08:08:44\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=1.83803, Score=0.80858 Threshold=0.32499999999999996\u001b[0m\n",
      "[ \u001b[32m2024-10-21 08:08:46\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=0, Loss=2.55542, LR=1.00000e-06\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11aa3cade60490eb51f4b5f38f69d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5919c0e72a4729aba48c3fbe0864c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-21 08:12:41\u001b[0m | \u001b[1mINFO ] Valid : Epoch=1, Loss=4.20555, Score=0.84081 Threshold=0.27499999999999997\u001b[0m\n",
      "[ \u001b[32m2024-10-21 08:12:46\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=1, Loss=6.40125, LR=4.98946e-07\u001b[0m\n",
      "[ \u001b[32m2024-10-21 08:12:50\u001b[0m | \u001b[1mINFO ] FOLD0 : First Training Done! -->> Best Score: 0.8408107492598499, Best Steps: 1000\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n\u001b[1;32m     18\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Create High-Quality Dataloader\u001b[39;00m\n\u001b[1;32m     22\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mdataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "oof_dfs = []\n",
    "best_steps, best_add_steps = [], []\n",
    "collate_fn = CollateFn(get_tokenizer(config), is_train=True)\n",
    "\n",
    "# この学習でベストなステップ数とOOFに対する予測値を取ることが目的\n",
    "for fold, (train_loader, valid_loader) in enumerate(dataloaders):\n",
    "    logger.info(f\"FOLD{fold} : Training Start...\")\n",
    "\n",
    "    # First Training\n",
    "    trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "    best_score, best_steps_, _ = trainer.train(train_loader, valid_loader)\n",
    "    if config.smooth_type == \"online\":\n",
    "        loss_soft_matrix = trainer.loss_fn.soft_matrix.clone()\n",
    "    best_steps.append(best_steps_)\n",
    "    logger.info(f\"FOLD{fold} : First Training Done! -->> Best Score: {best_score}, Best Steps: {best_steps_}\")\n",
    "\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Create High-Quality Dataloader\n",
    "    train_dataset = train_loader.dataset\n",
    "    train_dataset.drop_first_only_data()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=get_sampler(train_dataset),\n",
    "        batch_size=config.train_batch,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # Additional Training\n",
    "    trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "    if config.smooth_type == \"online\":\n",
    "        trainer.loss_fn.soft_matrix = loss_soft_matrix\n",
    "    best_score, best_add_steps_, oof_df = trainer.train(\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        retrain=True,\n",
    "        retrain_weight_name=f\"model_fold{fold}_best\",\n",
    "        retrain_best_score=best_score,\n",
    "    )\n",
    "    best_add_steps.append(best_add_steps_)\n",
    "    oof_df.write_parquet(config.output_path / f\"oof_fold{fold}.parquet\")\n",
    "    oof_dfs.append(oof_df)\n",
    "    logger.info(\n",
    "        f\"FOLD{fold} : Additional Training Done! -->> Best Score: {best_score}, Best Add Steps: {best_add_steps_}\"\n",
    "    )\n",
    "\n",
    "    del train_loader, valid_loader, train_dataset, trainer, oof_df\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del dataloaders\n",
    "gc.collect()\n",
    "\n",
    "# Save OOF\n",
    "oof_df = pl.concat(oof_dfs)\n",
    "oof_df.write_parquet(config.output_path / \"oof.parquet\")\n",
    "del oof_dfs\n",
    "gc.collect()\n",
    "\n",
    "# Get Best Negative Threshold\n",
    "best_score, best_th = get_best_negative_threshold(config, oof_df)\n",
    "message = f\"Overall OOF Best Score: {best_score}, Best Negative Threshold: {best_th}\"\n",
    "logger.info(message)\n",
    "config.negative_th = best_th.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-21 07:50:35\u001b[0m | \u001b[1mINFO ] Full Train : Training Start...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0a703d23b2438a8f2717e51ee795ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efecf7262854478a32cf19f2b8ae5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-21 07:51:08\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=0, Loss=3.26228, LR=1.00000e-06\u001b[0m\n",
      "[ \u001b[32m2024-10-21 07:51:08\u001b[0m | \u001b[1mINFO ] Full Train : First Training Done!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70051ed05a54492a191d7fc2f87644d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d1e9bfb1034b44a3a7c2a860b15313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.58 GiB total capacity; 23.20 GiB already allocated; 19.19 MiB free; 23.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39msmooth_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mloss_fn\u001b[38;5;241m.\u001b[39msoft_matrix \u001b[38;5;241m=\u001b[39m loss_soft_matrix\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain_weight_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_full\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_add_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull Train : Additional Training Done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_loader, trainer\n",
      "Cell \u001b[0;32mIn[10], line 100\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, valid_loader, retrain, retrain_weight_name, retrain_best_score, full_train, full_steps, eval_only)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_train_complete:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), n\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    102\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maccumulation_steps\n",
      "Cell \u001b[0;32mIn[10], line 202\u001b[0m, in \u001b[0;36mTrainer.forward_step\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    199\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m--> 202\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, labels)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, loss\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/exp/../src/model/detect_model.py:76\u001b[0m, in \u001b[0;36mDetectModel.forward\u001b[0;34m(self, input_ids, attention_mask, positions)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, positions):\n\u001b[1;32m     75\u001b[0m     x_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(positions)\n\u001b[0;32m---> 76\u001b[0m     x_bb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     x_bb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x_bb\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_hidden_states :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_bb \u001b[38;5;241m+\u001b[39m x_pos\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[0;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    499\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m         output_attentions,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:366\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    364\u001b[0m     attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[1;32m    365\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 366\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (layer_output, att_matrix)\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:332\u001b[0m, in \u001b[0;36mDebertaV2Output.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 332\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    334\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.58 GiB total capacity; 23.20 GiB already allocated; 19.19 MiB free; 23.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# # 全データ学習を行う\n",
    "if config.full_train:\n",
    "    full_steps = np.max(best_steps)\n",
    "    full_add_steps = np.max(best_add_steps)\n",
    "    logger.info(\"Full Train : Training Start...\")\n",
    "    train_loader = get_full_train_loader(config, data)\n",
    "\n",
    "    # First Training\n",
    "    trainer = Trainer(config, logger, save_suffix=\"\")\n",
    "    trainer.train(train_loader, valid_loader=None, full_train=True, full_steps=full_steps)\n",
    "    if config.smooth_type == \"online\":\n",
    "        loss_soft_matrix = trainer.loss_fn.soft_matrix.clone()\n",
    "    logger.info(\"Full Train : First Training Done!\")\n",
    "\n",
    "    # Create High-Quality Dataloader\n",
    "    train_dataset = train_loader.dataset\n",
    "    train_dataset.drop_first_only_data()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=get_sampler(train_dataset),\n",
    "        batch_size=config.train_batch,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # Additional Training\n",
    "    trainer = Trainer(config, logger, save_suffix=\"\")\n",
    "    if config.smooth_type == \"online\":\n",
    "        trainer.loss_fn.soft_matrix = loss_soft_matrix\n",
    "    trainer.train(\n",
    "        train_loader,\n",
    "        valid_loader=None,\n",
    "        retrain=True,\n",
    "        retrain_weight_name=\"model_full\",\n",
    "        full_train=True,\n",
    "        full_steps=full_add_steps,\n",
    "    )\n",
    "    logger.info(\"Full Train : Additional Training Done!\")\n",
    "\n",
    "    del train_loader, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_df = pl.read_parquet(Path(\"../data/output/058\") / \"oof.parquet\")\n",
    "# oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check Prefix Validity: 100%|██████████| 100/100 [00:00<00:00, 1373.59it/s]\n",
      "Check PII Validity: 100%|██████████| 413/413 [00:00<00:00, 618.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-21 11:58:02\u001b[0m | \u001b[1mINFO ] OOF Score after Post-Process: 0.88480\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# config.class_num = 13\n",
    "# config.negative_th = 0.42499999999999993\n",
    "# config.input_path = Path(\"../data/input\")\n",
    "# config.debug = True\n",
    "\n",
    "# # Post-Process\n",
    "# pred_df = get_pred_df(oof_df, config.class_num, negative_th=config.negative_th)\n",
    "# truth_df = get_truth_df(config, pred_df[\"document\"].unique().to_list(), convert_idx=True)\n",
    "\n",
    "# pper = PostProcessor(config)\n",
    "# pred_df = pper.post_process(pred_df)\n",
    "# score = evaluate_metric(pred_df, truth_df)\n",
    "# logger.info(f\"OOF Score after Post-Process: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
