{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../config/exp_087.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../config/exp_087.yaml\n",
    "exp: \"087\"\n",
    "seed: 10\n",
    "task_type: \"detect\"\n",
    "\n",
    "# data preprocess\n",
    "remove_prefix: true\n",
    "exter_dataset:\n",
    "  - [\"nicholas\", true]\n",
    "  - [\"mpware\", false]\n",
    "  - [\"pjma\", false]\n",
    "\n",
    "n_fold: 3\n",
    "use_fold: 3\n",
    "\n",
    "# dataset, dataloader\n",
    "add_newline_token: true\n",
    "max_length: 128\n",
    "train_stride: 96\n",
    "eval_stride: 64\n",
    "train_batch: 16\n",
    "eval_batch: 64\n",
    "\n",
    "# model\n",
    "model_path: \"microsoft/deberta-v3-large\"\n",
    "class_num: 8 # with prefix -> 13, without prefix -> 8\n",
    "lstm_type: \"none\"\n",
    "use_hidden_states: 2\n",
    "dropout: 0.10\n",
    "hidden_dropout: 0.10\n",
    "attention_dropout: 0.10\n",
    "reinit_layer_num: 0\n",
    "freeze_layer_num: 0\n",
    "\n",
    "# loss\n",
    "smooth_type: \"none\"\n",
    "smooth_ratio: 0.05\n",
    "smooth_pair: 0.05\n",
    "positive_class_weight: 10\n",
    "\n",
    "# optimizer\n",
    "optimizer_type: \"AdamW\"\n",
    "pretrained_lr: 1e-6\n",
    "head_lr: 1e-4\n",
    "weight_decay: 0.01\n",
    "betas: [0.9, 0.999]\n",
    "\n",
    "# scheduler\n",
    "scheduler_type: \"cosine_custom\"\n",
    "first_cycle_epochs: 1\n",
    "cycle_factor: 1\n",
    "num_warmup_steps: 100\n",
    "min_lr: 1e-9\n",
    "gamma: 1.0\n",
    "\n",
    "# training\n",
    "epochs: 4\n",
    "accumulation_steps: 2\n",
    "eval_steps: 1000\n",
    "negative_th: 0.660\n",
    "device: \"cuda\"\n",
    "amp: true\n",
    "ema: true\n",
    "ema_decay: 0.999\n",
    "ema_update_after_step: 8000\n",
    "\n",
    "# additional training\n",
    "add_train: true\n",
    "add_epochs: 2\n",
    "\n",
    "# full training\n",
    "full_train: true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.preprocess import DetectDataProvider\n",
    "from src.train import get_train_loaders\n",
    "from src.utils import TimeUtil, get_config, get_logger, seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = \"087\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 07:52:41\u001b[0m | \u001b[1mINFO ] exp:087 start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path(\"../config\"))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f\"exp:{exp} start\")\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nicholas', True], ['mpware', False], ['pjma', False]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.debug = True\n",
    "config.use_fold = 1\n",
    "config.exter_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpr = DetectDataProvider(config, \"train\")\n",
    "data = dpr.load_data()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for d in data:\n",
    "    labels.extend(d[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 299461, 7: 2821, 1: 2664, 5: 1188, 4: 438, 6: 348, 3: 316, 2: 295})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloaders = get_train_loaders(config, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.component_factory import ComponentFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "\n",
    "TARGET_PAIR_DICT = {\n",
    "    0: None,\n",
    "    1: 8,\n",
    "    2: None,\n",
    "    3: None,\n",
    "    4: 9,\n",
    "    5: 10,\n",
    "    6: 11,\n",
    "    7: 12,\n",
    "    8: 1,\n",
    "    9: 4,\n",
    "    10: 5,\n",
    "    11: 6,\n",
    "    12: 7,\n",
    "}\n",
    "\n",
    "\n",
    "class SmoothingCELoss(nn.Module):\n",
    "    def __init__(self, config: DictConfig, class_weight: list[float] | None = None):\n",
    "        super().__init__()\n",
    "        if class_weight is not None:\n",
    "            class_weight = torch.tensor(class_weight, dtype=torch.float, device=config.device)\n",
    "        self.loss = nn.CrossEntropyLoss(weight=class_weight)\n",
    "        self.device = config.device\n",
    "\n",
    "        self.class_num = config.class_num\n",
    "        if config.remove_prefix and config.smooth_type == \"weighted\":\n",
    "            config.smooth_type = \"normal\"\n",
    "        self.smooth_type = config.smooth_type\n",
    "        self.smooth_ratio = config.smooth_ratio\n",
    "        self.smooth_pair = config.smooth_pair\n",
    "        self.soft_matrix = self.get_soft_matrix()\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "        y_true = y_true.view(-1)\n",
    "\n",
    "        valid_idx = y_true != -1\n",
    "        y_pred = y_pred[valid_idx]\n",
    "        y_true = y_true[valid_idx]\n",
    "\n",
    "        y_true = self.get_soft_label(y_true)\n",
    "        return self.loss(y_pred, y_true)\n",
    "\n",
    "    def get_soft_label(self, y_true: torch.Tensor):\n",
    "        if self.smooth_type in [\"normal\", \"weighted\"]:\n",
    "            return self.soft_matrix[y_true]\n",
    "        else:\n",
    "            return y_true\n",
    "\n",
    "    def get_soft_matrix(self):\n",
    "        soft_matrix = torch.eye(self.class_num)\n",
    "\n",
    "        if self.smooth_type == \"normal\":\n",
    "            soft_matrix = soft_matrix * (1 - self.smooth_ratio) + self.smooth_ratio / self.class_num\n",
    "            return soft_matrix.to(self.device)\n",
    "\n",
    "        elif self.smooth_type == \"weighted\":\n",
    "            for c, c_p in TARGET_PAIR_DICT.items():\n",
    "                soft_label = soft_matrix[c]\n",
    "                if c_p is not None:\n",
    "                    soft_label[c_p] = self.smooth_pair\n",
    "\n",
    "                soft_label = torch.where(soft_label == 0, self.smooth_ratio / self.class_num, soft_label)\n",
    "                soft_label[c] = 1 - torch.sum(soft_label[soft_label != 1])\n",
    "                soft_matrix[c] = soft_label\n",
    "            return soft_matrix.to(self.device)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from src.model.detect_model import DetectModel\n",
    "\n",
    "# from src.model.classify_model import ClassifyModel\n",
    "# from src.train.loss import OnlineSmoothingCELoss, SmoothingCELoss\n",
    "from src.train.optimizer import get_optimizer\n",
    "from src.train.scheduler import get_scheduler\n",
    "\n",
    "\n",
    "class ComponentFactory:\n",
    "    # [TODO]要編集\n",
    "    @staticmethod\n",
    "    def get_model(config: DictConfig):\n",
    "        if config.task_type == \"detect\":\n",
    "            model = DetectModel(config)\n",
    "        elif config.task_type == \"classify\":\n",
    "            pass\n",
    "            # model = ClassifyModel(config)\n",
    "\n",
    "        if config.reinit_layer_num > 0:\n",
    "            model.reinit_layers(config.reinit_layer_num)\n",
    "        if config.freeze_layer_num > 0:\n",
    "            model.freeze_layers(config.freeze_layer_num)\n",
    "        return model\n",
    "\n",
    "    # [TODO]要編集\n",
    "    @staticmethod\n",
    "    def get_loss(config: DictConfig):\n",
    "        if config.task_type == \"detect\":\n",
    "            class_weight = [1] + [config.positive_class_weight] * (config.class_num - 1)\n",
    "            if config.smooth_type == \"online\":\n",
    "                loss_fn = OnlineSmoothingCELoss(config, class_weight=class_weight)\n",
    "            else:\n",
    "                loss_fn = SmoothingCELoss(config, class_weight=class_weight)\n",
    "        elif config.task_type == \"classify\":\n",
    "            # loss_fn = WeightedBCELoss()\n",
    "            pass\n",
    "        return loss_fn\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optimizer(config: DictConfig, model):\n",
    "        optimizer = get_optimizer(\n",
    "            model,\n",
    "            optimizer_type=config.optimizer_type,\n",
    "            pretrained_lr=config.pretrained_lr,\n",
    "            head_lr=config.head_lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=config.betas,\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def get_scheduler(config: DictConfig, optimizer: Optimizer, steps_per_epoch: int):\n",
    "        total_steps = (config.epochs - 1) * steps_per_epoch  # 1epoch目はlrを減衰させない\n",
    "        if config.scheduler_type == \"linear\":\n",
    "            scheduler_args = {\n",
    "                \"num_warmup_steps\": config.num_warmup_steps,\n",
    "                \"num_training_steps\": total_steps,\n",
    "            }\n",
    "        elif config.scheduler_type == \"cosine\":\n",
    "            scheduler_args = {\n",
    "                \"num_warmup_steps\": config.num_warmup_steps,\n",
    "                \"num_training_steps\": total_steps,\n",
    "                \"num_cycles\": config.num_cycles,\n",
    "            }\n",
    "        elif config.scheduler_type == \"cosine_custom\":\n",
    "            first_cycle_steps = config.first_cycle_epochs * steps_per_epoch\n",
    "            scheduler_args = {\n",
    "                \"first_cycle_steps\": first_cycle_steps,\n",
    "                \"cycle_factor\": config.cycle_factor,\n",
    "                \"num_warmup_steps\": config.num_warmup_steps,\n",
    "                \"min_lr\": config.min_lr,\n",
    "                \"gamma\": config.gamma,\n",
    "            }\n",
    "        elif config.scheduler_type == \"reduce_on_plateau\":\n",
    "            scheduler_args = {\n",
    "                \"mode\": config.mode,\n",
    "                \"factor\": config.factor,\n",
    "                \"patience\": config.patience,\n",
    "                \"min_lr\": config.min_lr,\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid scheduler_type: {config.scheduler_type}\")\n",
    "\n",
    "        scheduler = get_scheduler(optimizer, scheduler_type=config.scheduler_type, scheduler_args=scheduler_args)\n",
    "        return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loguru\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from torch.cuda import amp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.train.ema import ModelEmaV3\n",
    "from src.utils.competition_utils import (\n",
    "    get_char2org_df,\n",
    "    get_char_pred_df,\n",
    "    get_original_token_df,\n",
    "    get_pred_df,\n",
    "    get_truth_df,\n",
    "    restore_prefix,\n",
    ")\n",
    "from src.utils.metric import evaluate_metric\n",
    "from src.utils.utils import AverageMeter, clean_message\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        if self.config.ema:\n",
    "            self.model_ema = ModelEmaV3(\n",
    "                self.model,\n",
    "                decay=config.ema_decay,\n",
    "                update_after_step=config.ema_update_after_step,\n",
    "                device=config.device,\n",
    "            )\n",
    "\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "        self.grad_scaler = amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_global_steps: int = 0,\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            score, loss, oof_df = self.valid_evaluate(valid_loader, epoch=-1, load_best_weight=True)\n",
    "            return score, -1, oof_df\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(self.config, self.model)\n",
    "        self.scheduler = ComponentFactory.get_scheduler(self.config, self.optimizer, steps_per_epoch=len(train_loader))\n",
    "\n",
    "        global_steps = 0\n",
    "        update_steps = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.model.load_state_dict(torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\"))\n",
    "            global_steps = retrain_global_steps\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        # 学習ループの開始\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            # 1epoch目はbackboneをfreezeする\n",
    "            if epoch == 0:\n",
    "                self.model.freeze_backbone(config.reinit_layer_num)\n",
    "            elif epoch == 1:\n",
    "                self.model.unfreeze_backbone(config.freeze_layer_num)\n",
    "\n",
    "            iterations = tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(self.model, data)\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                loss = loss / self.config.accumulation_steps\n",
    "                self.grad_scaler.scale(loss).backward()\n",
    "                global_steps += 1\n",
    "\n",
    "                if global_steps % self.config.accumulation_steps == 0:\n",
    "                    self.grad_scaler.step(self.optimizer)\n",
    "                    self.grad_scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    if self.config.ema:\n",
    "                        self.model_ema.update(self.model, global_steps)\n",
    "\n",
    "                    # backboneの学習が始まってからschedulerを適用\n",
    "                    if epoch > 0:\n",
    "                        self.scheduler.step()\n",
    "                        update_steps += 1\n",
    "\n",
    "                if global_steps % self.config.eval_steps == 0:\n",
    "                    score, loss, oof_df = self.valid_evaluate(valid_loader, epoch, load_best_weight=False)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_steps = global_steps\n",
    "                        best_oof = oof_df\n",
    "                        parameters = self.model_ema.module.state_dict() if self.config.ema else self.model.state_dict()\n",
    "                        torch.save(\n",
    "                            parameters,\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "            if self.config.smooth_type == \"online\":\n",
    "                self.loss_fn.update_soft_matrix()\n",
    "\n",
    "        return best_score, best_steps, best_oof\n",
    "\n",
    "    def valid_evaluate(self, valid_loader: DataLoader, epoch: int, load_best_weight: bool = False):\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\"))\n",
    "\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = tqdm(valid_loader, total=len(valid_loader)) if self.detail_pbar else valid_loader\n",
    "            for data in iterations:\n",
    "                if load_best_weight or not self.config.ema:\n",
    "                    out, loss = self.forward_step(self.model, data)\n",
    "                else:\n",
    "                    out, loss = self.forward_step(self.model_ema, data)\n",
    "\n",
    "                self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                preds.extend(F.softmax(out, dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "        oof_df = self.get_oof_df(preds, valid_loader)\n",
    "        pred_df = get_pred_df(oof_df, self.config.class_num, self.config.negative_th)\n",
    "        if self.config.remove_prefix:\n",
    "            pred_df = restore_prefix(self.config, pred_df)\n",
    "\n",
    "        truth_df = get_truth_df(self.config, pred_df[\"document\"].unique().to_list(), convert_idx=True)\n",
    "        score = evaluate_metric(pred_df, truth_df)\n",
    "\n",
    "        loss = self.valid_loss.avg\n",
    "        message = f\"\"\"\n",
    "            Valid :\n",
    "                Epoch={epoch},\n",
    "                Loss={loss:.5f},\n",
    "                Score={score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, loss, oof_df\n",
    "\n",
    "    def forward_step(self, model: nn.Module, data: torch.Tensor):\n",
    "        input_ids, attention_mask, positions_feats, labels = data\n",
    "        input_ids = input_ids.to(self.config.device)\n",
    "        attention_mask = attention_mask.to(self.config.device)\n",
    "        positions_feats = positions_feats.to(self.config.device)\n",
    "        labels = labels.to(self.config.device)\n",
    "        with amp.autocast(enabled=self.config.amp):\n",
    "            out = model(input_ids, attention_mask, positions_feats)\n",
    "            loss = self.loss_fn(out, labels)\n",
    "        return out, loss\n",
    "\n",
    "    def get_oof_df(self, preds: list[list[float]], valid_loader: DataLoader):\n",
    "        char_pred_df = get_char_pred_df(\n",
    "            preds,\n",
    "            valid_loader.dataset.overlap_doc_ids,\n",
    "            valid_loader.dataset.offset_mapping,\n",
    "            class_num=self.config.class_num,\n",
    "        )\n",
    "        char2org_df = get_char2org_df(\n",
    "            valid_loader.dataset.doc_ids,\n",
    "            valid_loader.dataset.full_texts,\n",
    "            valid_loader.dataset.org_tokens,\n",
    "            valid_loader.dataset.whitespaces,\n",
    "        )\n",
    "        oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
    "        oof_df = (\n",
    "            oof_df.filter(pl.col(\"token_index\") != -1)\n",
    "            .group_by(\"document\", \"token_index\")\n",
    "            .agg([pl.col(f\"pred_{i}\").mean() for i in range(self.config.class_num)])\n",
    "        )\n",
    "        return oof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 07:49:45\u001b[0m | \u001b[1mINFO ] \n",
      " FOLD0 : Training Start \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd874ab56b624a7c837d6c6e970376a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2348cb1bf1344db1a0774f4a0a6a7df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 07:50:24\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=0, Loss=2.07022, LR=1.00000e-09\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09e04380a654437b54600b27862d2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0690dbf0944fd68ca3777dc0de93bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DuplicateError",
     "evalue": "column with name 'pred_{i}' has more than one occurrences",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m FOLD\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : Training Start \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config, logger, save_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m best_score, best_steps, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # 追加の学習を行う準備をする\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# train_dataset = train_loader.dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# train_dataset.drop_first_only_data()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     retrain_best_score=best_score,\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 111\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, valid_loader, retrain, retrain_weight_name, retrain_global_steps, retrain_best_score, eval_only)\u001b[0m\n\u001b[1;32m    108\u001b[0m         update_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m     score, loss, oof_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_best_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n\u001b[1;32m    113\u001b[0m         best_score \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[0;32mIn[13], line 153\u001b[0m, in \u001b[0;36mTrainer.valid_evaluate\u001b[0;34m(self, valid_loader, epoch, load_best_weight)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loss\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), n\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    151\u001b[0m         preds\u001b[38;5;241m.\u001b[39mextend(F\u001b[38;5;241m.\u001b[39msoftmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m--> 153\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_oof_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m get_pred_df(oof_df, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclass_num, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnegative_th)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mremove_prefix:\n",
      "Cell \u001b[0;32mIn[13], line 183\u001b[0m, in \u001b[0;36mTrainer.get_oof_df\u001b[0;34m(self, preds, valid_loader)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_oof_df\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]], valid_loader: DataLoader):\n\u001b[0;32m--> 183\u001b[0m     char_pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_char_pred_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap_doc_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     char2org_df \u001b[38;5;241m=\u001b[39m get_char2org_df(\n\u001b[1;32m    190\u001b[0m         valid_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdoc_ids,\n\u001b[1;32m    191\u001b[0m         valid_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfull_texts,\n\u001b[1;32m    192\u001b[0m         valid_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39morg_tokens,\n\u001b[1;32m    193\u001b[0m         valid_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mwhitespaces,\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m     oof_df \u001b[38;5;241m=\u001b[39m char_pred_df\u001b[38;5;241m.\u001b[39mjoin(char2org_df, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:201\u001b[0m, in \u001b[0;36mget_char_pred_df\u001b[0;34m(preds, overlap_doc_ids, offset_mappings, class_num)\u001b[0m\n\u001b[1;32m    198\u001b[0m         char_preds\u001b[38;5;241m.\u001b[39mextend([pred[i]] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mrange\u001b[39m(start, end)))\n\u001b[1;32m    200\u001b[0m char_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(char_preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (char_len, class_num)\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m char_pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred_\u001b[39;49m\u001b[38;5;132;43;01m{i}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m char_pred_df \u001b[38;5;241m=\u001b[39m char_pred_df\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[1;32m    203\u001b[0m     document\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mSeries(doc_ids),\n\u001b[1;32m    204\u001b[0m     char_idx\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mSeries(char_indices),\n\u001b[1;32m    205\u001b[0m )\n\u001b[1;32m    206\u001b[0m char_pred_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    207\u001b[0m     char_pred_df\u001b[38;5;241m.\u001b[39mgroup_by(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;241m.\u001b[39magg([pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(class_num)])\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;241m.\u001b[39mwith_columns(document\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mInt32), char_idx\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mInt32))\n\u001b[1;32m    211\u001b[0m )\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/polars/dataframe/frame.py:381\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, schema, schema_overrides, strict, orient, infer_schema_length, nan_to_null)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m series_to_pydf(\n\u001b[1;32m    377\u001b[0m         data, schema\u001b[38;5;241m=\u001b[39mschema, schema_overrides\u001b[38;5;241m=\u001b[39mschema_overrides, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _check_for_numpy(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy_to_pydf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _check_for_pyarrow(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pa\u001b[38;5;241m.\u001b[39mTable):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m arrow_to_pydf(\n\u001b[1;32m    392\u001b[0m         data, schema\u001b[38;5;241m=\u001b[39mschema, schema_overrides\u001b[38;5;241m=\u001b[39mschema_overrides, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/polars/_utils/construction/dataframe.py:1318\u001b[0m, in \u001b[0;36mnumpy_to_pydf\u001b[0;34m(data, schema, schema_overrides, orient, strict, nan_to_null)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         data_series \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1305\u001b[0m             pl\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[1;32m   1306\u001b[0m                 name\u001b[38;5;241m=\u001b[39mcolumn_names[i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_columns)\n\u001b[1;32m   1315\u001b[0m         ]\n\u001b[1;32m   1317\u001b[0m data_series \u001b[38;5;241m=\u001b[39m _handle_columns_arg(data_series, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m-> 1318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_series\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mDuplicateError\u001b[0m: column with name 'pred_{i}' has more than one occurrences"
     ]
    }
   ],
   "source": [
    "from src.train.train_utils import CollateFn, get_sampler, get_tokenizer\n",
    "\n",
    "oof_dfs = []\n",
    "tokenizer = get_tokenizer(config)\n",
    "collate_fn = CollateFn(tokenizer, is_train=True)\n",
    "\n",
    "for fold, (train_loader, valid_loader) in enumerate(dataloaders):\n",
    "    logger.info(f\"\\n FOLD{fold} : Training Start \\n\")\n",
    "    trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "\n",
    "    best_score, best_steps, _ = trainer.train(train_loader, valid_loader)\n",
    "\n",
    "    break\n",
    "\n",
    "    # # 追加の学習を行う準備をする\n",
    "    # train_dataset = train_loader.dataset\n",
    "    # train_dataset.drop_first_only_data()\n",
    "    # train_loader = DataLoader(\n",
    "    #     train_dataset,\n",
    "    #     sampler=get_sampler(train_dataset),\n",
    "    #     batch_size=config.batch_size,\n",
    "    #     collate_fn=collate_fn,\n",
    "    #     pin_memory=True,\n",
    "    #     drop_last=True,\n",
    "    # )\n",
    "\n",
    "    # trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "    # best_score, best_steps, oof_df = trainer.train(\n",
    "    #     train_loader,\n",
    "    #     valid_loader,\n",
    "    #     retrain=True,\n",
    "    #     retrain_weight_name=f\"model_fold{fold}_best\",\n",
    "    #     retrain_best_score=best_score,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train over folds\n",
    "oof_dfs = []\n",
    "best_steps_list, best_add_steps_list = [], []\n",
    "for fold, (train_loader, valid_loader) in enumerate(dataloaders):\n",
    "    logger.info(f\"\\n FOLD{fold} : Training Start \\n\")\n",
    "    model = get_model(config)\n",
    "    optimizer = get_optimizer(config, model)\n",
    "    oof_df, score, best_steps, best_add_steps = train_model(\n",
    "        config,\n",
    "        model,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        optimizer,\n",
    "        logger,\n",
    "        fold,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "    oof_df.write_parquet(config.oof_path / f\"oof_fold{fold}{suffix}.parquet\")\n",
    "    oof_dfs.append(oof_df)\n",
    "    best_score, best_th = get_best_negative_threshold(config, oof_df)\n",
    "    config.negative_th = best_th\n",
    "    message = f\"FOLD: {fold}, Steps: {best_steps} + {best_add_steps}, Best Score: {best_score}, Best Negative Threshold: {best_th}\"\n",
    "    logger.info(message)\n",
    "    best_steps_list.append(best_steps)\n",
    "    best_add_steps_list.append(best_add_steps)\n",
    "\n",
    "    del train_loader, valid_loader, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "del dataloaders\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save oof\n",
    "oof_df = pl.concat(oof_dfs)\n",
    "oof_df.write_parquet(config.oof_path / f\"oof_{config.exp}{suffix}.parquet\")\n",
    "\n",
    "# get best threshold\n",
    "best_score, best_th = get_best_negative_threshold(config, oof_df)\n",
    "message = f\"Overall OOF Best Score: {best_score}, Best Negative Threshold: {best_th}\"\n",
    "logger.info(message)\n",
    "config.negative_th = best_th\n",
    "\n",
    "# full train\n",
    "if config.full_train:\n",
    "    full_train_steps, full_train_add_steps = np.max(best_steps_list), np.max(best_add_steps_list)\n",
    "    logger.info(f\"\\n Full Train : Training Start, Num of Steps : {full_train_steps} + {full_train_add_steps}\\n\")\n",
    "    train_loader = get_full_train_loader(config, train_data)\n",
    "    model = get_model(config)\n",
    "    optimizer = get_optimizer(config, model)\n",
    "    full_train_model(config, model, train_loader, optimizer, full_train_steps, full_train_add_steps, logger, suffix)\n",
    "    message = \"Full Train Completed\"\n",
    "    logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
