{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../config/exp_087.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../config/exp_087.yaml\n",
    "exp: \"087\"\n",
    "seed: 10\n",
    "task_type: \"detect\"\n",
    "\n",
    "# data preprocess\n",
    "remove_prefix: true\n",
    "exter_dataset:\n",
    "  - [\"nicholas\", true]\n",
    "  - [\"mpware\", false]\n",
    "  - [\"pjma\", false]\n",
    "\n",
    "n_fold: 3\n",
    "use_fold: 3\n",
    "\n",
    "# dataset, dataloader\n",
    "add_newline_token: true\n",
    "max_length: 128\n",
    "train_stride: 96\n",
    "eval_stride: 64\n",
    "train_batch: 16\n",
    "eval_batch: 64\n",
    "\n",
    "# model\n",
    "model_path: \"microsoft/deberta-v3-large\"\n",
    "class_num: 8 # with prefix -> 13, without prefix -> 8\n",
    "lstm_type: \"none\"\n",
    "use_hidden_states: 2\n",
    "dropout: 0.10\n",
    "hidden_dropout: 0.10\n",
    "attention_dropout: 0.10\n",
    "reinit_layer_num: 0\n",
    "freeze_layer_num: 0\n",
    "\n",
    "# loss\n",
    "smooth_type: \"none\"\n",
    "smooth_ratio: 0.05\n",
    "smooth_pair: 0.05\n",
    "positive_class_weight: 10\n",
    "\n",
    "# optimizer\n",
    "optimizer_type: \"AdamW\"\n",
    "pretrained_lr: 1e-6\n",
    "head_lr: 1e-4\n",
    "weight_decay: 0.01\n",
    "betas: [0.9, 0.999]\n",
    "\n",
    "# scheduler\n",
    "scheduler_type: \"cosine_custom\"\n",
    "first_cycle_epochs: 1\n",
    "cycle_factor: 1\n",
    "num_warmup_steps: 100\n",
    "min_lr: 1e-9\n",
    "gamma: 1.0\n",
    "\n",
    "# training\n",
    "epochs: 4\n",
    "accumulation_steps: 2\n",
    "eval_steps: 1000\n",
    "negative_th: 0.660\n",
    "device: \"cuda\"\n",
    "amp: true\n",
    "ema: true\n",
    "ema_decay: 0.999\n",
    "ema_update_after_step: 8000\n",
    "\n",
    "# additional training\n",
    "add_train: true\n",
    "add_epochs: 2\n",
    "\n",
    "# full training\n",
    "full_train: true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.preprocess import DetectDataProvider\n",
    "from src.train import get_train_loaders\n",
    "from src.utils import TimeUtil, get_config, get_logger, seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = \"087\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:03:16\u001b[0m | \u001b[1mINFO ] exp:087 start\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path(\"../config\"))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f\"exp:{exp} start\")\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nicholas', True], ['mpware', False], ['pjma', False]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.debug = True\n",
    "config.use_fold = 1\n",
    "config.exter_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpr = DetectDataProvider(config, \"train\")\n",
    "data = dpr.load_data()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for d in data:\n",
    "    labels.extend(d[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 299461, 7: 2821, 1: 2664, 5: 1188, 4: 438, 6: 348, 3: 316, 2: 295})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloaders = get_train_loaders(config, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.component_factory import ComponentFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "\n",
    "TARGET_PAIR_DICT = {\n",
    "    0: None,\n",
    "    1: 8,\n",
    "    2: None,\n",
    "    3: None,\n",
    "    4: 9,\n",
    "    5: 10,\n",
    "    6: 11,\n",
    "    7: 12,\n",
    "    8: 1,\n",
    "    9: 4,\n",
    "    10: 5,\n",
    "    11: 6,\n",
    "    12: 7,\n",
    "}\n",
    "\n",
    "\n",
    "class SmoothingCELoss(nn.Module):\n",
    "    def __init__(self, config: DictConfig, class_weight: list[float] | None = None):\n",
    "        super().__init__()\n",
    "        if class_weight is not None:\n",
    "            class_weight = torch.tensor(class_weight, dtype=torch.float, device=config.device)\n",
    "        self.loss = nn.CrossEntropyLoss(weight=class_weight)\n",
    "        self.device = config.device\n",
    "\n",
    "        self.class_num = config.class_num\n",
    "        if config.remove_prefix and config.smooth_type == \"weighted\":\n",
    "            config.smooth_type = \"normal\"\n",
    "        self.smooth_type = config.smooth_type\n",
    "        self.smooth_ratio = config.smooth_ratio\n",
    "        self.smooth_pair = config.smooth_pair\n",
    "        self.soft_matrix = self.get_soft_matrix()\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "        y_true = y_true.view(-1)\n",
    "\n",
    "        valid_idx = y_true != -1\n",
    "        y_pred = y_pred[valid_idx]\n",
    "        y_true = y_true[valid_idx]\n",
    "\n",
    "        y_true = self.get_soft_label(y_true)\n",
    "        return self.loss(y_pred, y_true)\n",
    "\n",
    "    def get_soft_label(self, y_true: torch.Tensor):\n",
    "        if self.smooth_type in [\"normal\", \"weighted\"]:\n",
    "            return self.soft_matrix[y_true]\n",
    "        else:\n",
    "            return y_true\n",
    "\n",
    "    def get_soft_matrix(self):\n",
    "        soft_matrix = torch.eye(self.class_num)\n",
    "\n",
    "        if self.smooth_type == \"normal\":\n",
    "            soft_matrix = soft_matrix * (1 - self.smooth_ratio) + self.smooth_ratio / self.class_num\n",
    "            return soft_matrix.to(self.device)\n",
    "\n",
    "        elif self.smooth_type == \"weighted\":\n",
    "            for c, c_p in TARGET_PAIR_DICT.items():\n",
    "                soft_label = soft_matrix[c]\n",
    "                if c_p is not None:\n",
    "                    soft_label[c_p] = self.smooth_pair\n",
    "\n",
    "                soft_label = torch.where(soft_label == 0, self.smooth_ratio / self.class_num, soft_label)\n",
    "                soft_label[c] = 1 - torch.sum(soft_label[soft_label != 1])\n",
    "                soft_matrix[c] = soft_label\n",
    "            return soft_matrix.to(self.device)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from src.model.detect_model import DetectModel\n",
    "\n",
    "# from src.model.classify_model import ClassifyModel\n",
    "# from src.train.loss import OnlineSmoothingCELoss, SmoothingCELoss\n",
    "from src.train.optimizer import get_optimizer\n",
    "from src.train.scheduler import get_scheduler\n",
    "\n",
    "\n",
    "class ComponentFactory:\n",
    "    # [TODO]要編集\n",
    "    @staticmethod\n",
    "    def get_model(config: DictConfig):\n",
    "        if config.task_type == \"detect\":\n",
    "            model = DetectModel(config)\n",
    "        elif config.task_type == \"classify\":\n",
    "            pass\n",
    "            # model = ClassifyModel(config)\n",
    "\n",
    "        if config.reinit_layer_num > 0:\n",
    "            model.reinit_layers(config.reinit_layer_num)\n",
    "        if config.freeze_layer_num > 0:\n",
    "            model.freeze_layers(config.freeze_layer_num)\n",
    "        return model\n",
    "\n",
    "    # [TODO]要編集\n",
    "    @staticmethod\n",
    "    def get_loss(config: DictConfig):\n",
    "        if config.task_type == \"detect\":\n",
    "            class_weight = [1] + [config.positive_class_weight] * (config.class_num - 1)\n",
    "            if config.smooth_type == \"online\":\n",
    "                loss_fn = OnlineSmoothingCELoss(config, class_weight=class_weight)\n",
    "            else:\n",
    "                loss_fn = SmoothingCELoss(config, class_weight=class_weight)\n",
    "        elif config.task_type == \"classify\":\n",
    "            # loss_fn = WeightedBCELoss()\n",
    "            pass\n",
    "        return loss_fn\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optimizer(config: DictConfig, model):\n",
    "        optimizer = get_optimizer(\n",
    "            model,\n",
    "            optimizer_type=config.optimizer_type,\n",
    "            pretrained_lr=config.pretrained_lr,\n",
    "            head_lr=config.head_lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=config.betas,\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def get_scheduler(config: DictConfig, optimizer: Optimizer, steps_per_epoch: int):\n",
    "        total_steps = (config.epochs - 1) * steps_per_epoch  # 1epoch目はlrを減衰させない\n",
    "        if config.scheduler_type == \"linear\":\n",
    "            scheduler_args = {\n",
    "                \"num_warmup_steps\": config.num_warmup_steps,\n",
    "                \"num_training_steps\": total_steps,\n",
    "            }\n",
    "        elif config.scheduler_type == \"cosine\":\n",
    "            scheduler_args = {\n",
    "                \"num_warmup_steps\": config.num_warmup_steps,\n",
    "                \"num_training_steps\": total_steps,\n",
    "                \"num_cycles\": config.num_cycles,\n",
    "            }\n",
    "        elif config.scheduler_type == \"cosine_custom\":\n",
    "            first_cycle_steps = config.first_cycle_epochs * steps_per_epoch\n",
    "            scheduler_args = {\n",
    "                \"first_cycle_steps\": first_cycle_steps,\n",
    "                \"cycle_factor\": config.cycle_factor,\n",
    "                \"num_warmup_steps\": config.num_warmup_steps,\n",
    "                \"min_lr\": config.min_lr,\n",
    "                \"gamma\": config.gamma,\n",
    "            }\n",
    "        elif config.scheduler_type == \"reduce_on_plateau\":\n",
    "            scheduler_args = {\n",
    "                \"mode\": config.mode,\n",
    "                \"factor\": config.factor,\n",
    "                \"patience\": config.patience,\n",
    "                \"min_lr\": config.min_lr,\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid scheduler_type: {config.scheduler_type}\")\n",
    "\n",
    "        scheduler = get_scheduler(optimizer, scheduler_type=config.scheduler_type, scheduler_args=scheduler_args)\n",
    "        return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fbeta(tp: int, fp: int, fn: int, beta: float = 5.0):\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0.0\n",
    "    score = (1 + (beta**2)) * precision * recall / (beta**2 * precision + recall)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_metric(\n",
    "    pred_df: pl.DataFrame,\n",
    "    truth_df: pl.DataFrame,\n",
    ") -> float:\n",
    "    truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n",
    "    truth_df = truth_df.with_columns(pred=pl.col(\"pred\").fill_null(0))\n",
    "\n",
    "    tp = len(truth_df.filter((pl.col(\"label\") != 0) & (pl.col(\"label\") == pl.col(\"pred\"))))\n",
    "    fp = len(truth_df.filter((pl.col(\"label\") == 0) & (pl.col(\"pred\") != 0)))\n",
    "    fn = len(truth_df.filter((pl.col(\"label\") != 0) & (pl.col(\"pred\") == 0)))\n",
    "    fp_fn = len(truth_df.filter((pl.col(\"label\") != 0) & (pl.col(\"pred\") != 0) & (pl.col(\"label\") != pl.col(\"pred\"))))\n",
    "    score = calculate_fbeta(tp, fp + fp_fn, fn + fp_fn)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loguru\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from torch.cuda import amp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.train.ema import ModelEmaV3\n",
    "from src.utils.competition_utils import (\n",
    "    get_char2org_df,\n",
    "    get_char_pred_df,\n",
    "    get_original_token_df,\n",
    "    get_pred_df,\n",
    "    get_truth_df,\n",
    "    restore_prefix,\n",
    ")\n",
    "from src.utils.utils import AverageMeter, clean_message\n",
    "\n",
    "# from src.utils.metric import evaluate_metric\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        if self.config.ema:\n",
    "            self.model_ema = ModelEmaV3(\n",
    "                self.model,\n",
    "                decay=config.ema_decay,\n",
    "                update_after_step=config.ema_update_after_step,\n",
    "                device=config.device,\n",
    "            )\n",
    "\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "        self.grad_scaler = amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_global_steps: int = 0,\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            score, loss, oof_df = self.valid_evaluate(valid_loader, epoch=-1, load_best_weight=True)\n",
    "            return score, -1, oof_df\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(self.config, self.model)\n",
    "        self.scheduler = ComponentFactory.get_scheduler(self.config, self.optimizer, steps_per_epoch=len(train_loader))\n",
    "\n",
    "        global_steps = 0\n",
    "        update_steps = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.model.load_state_dict(torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\"))\n",
    "            global_steps = retrain_global_steps\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        # 学習ループの開始\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            # 1epoch目はbackboneをfreezeする\n",
    "            if epoch == 0:\n",
    "                self.model.freeze_backbone(config.reinit_layer_num)\n",
    "            elif epoch == 1:\n",
    "                self.model.unfreeze_backbone(config.freeze_layer_num)\n",
    "\n",
    "            iterations = tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(self.model, data)\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                loss = loss / self.config.accumulation_steps\n",
    "                self.grad_scaler.scale(loss).backward()\n",
    "                global_steps += 1\n",
    "\n",
    "                if global_steps % self.config.accumulation_steps == 0:\n",
    "                    self.grad_scaler.step(self.optimizer)\n",
    "                    self.grad_scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    if self.config.ema:\n",
    "                        self.model_ema.update(self.model, global_steps)\n",
    "\n",
    "                    # backboneの学習が始まってからschedulerを適用\n",
    "                    if epoch > 0:\n",
    "                        self.scheduler.step()\n",
    "                        update_steps += 1\n",
    "\n",
    "                if global_steps % self.config.eval_steps == 0:\n",
    "                    score, loss, oof_df = self.valid_evaluate(valid_loader, epoch, load_best_weight=False)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_steps = global_steps\n",
    "                        best_oof = oof_df\n",
    "                        parameters = self.model_ema.module.state_dict() if self.config.ema else self.model.state_dict()\n",
    "                        torch.save(\n",
    "                            parameters,\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "            if self.config.smooth_type == \"online\":\n",
    "                self.loss_fn.update_soft_matrix()\n",
    "\n",
    "        return best_score, best_steps, best_oof\n",
    "\n",
    "    def valid_evaluate(self, valid_loader: DataLoader, epoch: int, load_best_weight: bool = False):\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\"))\n",
    "\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = tqdm(valid_loader, total=len(valid_loader)) if self.detail_pbar else valid_loader\n",
    "            for data in iterations:\n",
    "                if load_best_weight or not self.config.ema:\n",
    "                    out, loss = self.forward_step(self.model, data)\n",
    "                else:\n",
    "                    out, loss = self.forward_step(self.model_ema, data)\n",
    "\n",
    "                self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                preds.extend(F.softmax(out, dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "        oof_df = self.get_oof_df(preds, valid_loader)\n",
    "        pred_df = get_pred_df(oof_df, self.config.class_num, self.config.negative_th)\n",
    "        if self.config.remove_prefix:\n",
    "            pred_df = restore_prefix(self.config, pred_df)\n",
    "\n",
    "        truth_df = get_truth_df(self.config, pred_df[\"document\"].unique().to_list(), convert_idx=True)\n",
    "        score = evaluate_metric(pred_df, truth_df)\n",
    "\n",
    "        loss = self.valid_loss.avg\n",
    "        message = f\"\"\"\n",
    "            Valid :\n",
    "                Epoch={epoch},\n",
    "                Loss={loss:.5f},\n",
    "                Score={score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, loss, oof_df\n",
    "\n",
    "    def forward_step(self, model: nn.Module, data: torch.Tensor):\n",
    "        input_ids, attention_mask, positions_feats, labels = data\n",
    "        input_ids = input_ids.to(self.config.device)\n",
    "        attention_mask = attention_mask.to(self.config.device)\n",
    "        positions_feats = positions_feats.to(self.config.device)\n",
    "        labels = labels.to(self.config.device)\n",
    "        with amp.autocast(enabled=self.config.amp):\n",
    "            out = model(input_ids, attention_mask, positions_feats)\n",
    "            loss = self.loss_fn(out, labels)\n",
    "        return out, loss\n",
    "\n",
    "    def get_oof_df(self, preds: list[list[float]], valid_loader: DataLoader):\n",
    "        char_pred_df = get_char_pred_df(\n",
    "            preds,\n",
    "            valid_loader.dataset.overlap_doc_ids,\n",
    "            valid_loader.dataset.offset_mapping,\n",
    "            class_num=self.config.class_num,\n",
    "        )\n",
    "        char2org_df = get_char2org_df(\n",
    "            valid_loader.dataset.doc_ids,\n",
    "            valid_loader.dataset.full_texts,\n",
    "            valid_loader.dataset.org_tokens,\n",
    "            valid_loader.dataset.whitespaces,\n",
    "        )\n",
    "        oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
    "        oof_df = (\n",
    "            oof_df.filter(pl.col(\"token_idx\") != -1)\n",
    "            .group_by(\"document\", \"token_idx\")\n",
    "            .agg([pl.col(f\"pred_{i}\").mean() for i in range(self.config.class_num)])\n",
    "        )\n",
    "        return oof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:33:44\u001b[0m | \u001b[1mINFO ] \n",
      " FOLD0 : Training Start \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a88ce1b4314e4db57261746643a1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e83684fbb34f83b12961a37d59455f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9131b19aedd464aa486a88a0350074b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:33:49\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19669, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95920a700784303b4a4d9a6c93d3c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:33:54\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19668, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad73e7c3a584fa9b47733572f621fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:33:57\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19668, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fb768e64d7423eba9a3c997554534b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:34:01\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19668, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dc067098c84d36a1a082a377f38b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:34:05\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19668, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b263094fac441fb4946ce5ea13509b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:34:08\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19667, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7590132adbb34bcc93198fbcded1ea2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:34:12\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19667, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fa0f0e033d4722bf7261395f89c1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:34:16\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19667, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e525bfaaad845808e0ecf0501dead89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/1564953696.py:195: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  oof_df = char_pred_df.join(char2org_df, on=[\"document\", \"char_idx\"], how=\"left\")\n",
      "/root/kaggle-pii-5th-place-solution/exp/../src/utils/competition_utils.py:248: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  pred_df = pred_df.with_columns(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-17 09:34:19\u001b[0m | \u001b[1mINFO ] Valid : Epoch=0, Loss=2.19667, Score=0.02382\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86673/2576574933.py:14: DeprecationWarning: The default coalesce behavior of left join will change to `False` in the next breaking release. Pass `coalesce=True` to keep the current behavior and silence this warning.\n",
      "  truth_df = truth_df.join(pred_df, on=[\"document\", \"token_idx\"], how=\"left\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# First Training\u001b[39;00m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config, logger, save_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m best_score, best_steps, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# High Quality Data Training\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# train_dataset = train_loader.dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# train_dataset.drop_first_only_data()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     retrain_best_score=best_score,\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 91\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, valid_loader, retrain, retrain_weight_name, retrain_global_steps, retrain_best_score, eval_only)\u001b[0m\n\u001b[1;32m     89\u001b[0m iterations \u001b[38;5;241m=\u001b[39m tqdm(train_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetail_pbar \u001b[38;5;28;01melse\u001b[39;00m train_loader\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m iterations:\n\u001b[0;32m---> 91\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), n\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     93\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maccumulation_steps\n",
      "Cell \u001b[0;32mIn[22], line 178\u001b[0m, in \u001b[0;36mTrainer.forward_step\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    176\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m--> 178\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, labels)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, loss\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/exp/../src/model/detect_model.py:76\u001b[0m, in \u001b[0;36mDetectModel.forward\u001b[0;34m(self, input_ids, attention_mask, positions)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, positions):\n\u001b[1;32m     75\u001b[0m     x_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(positions)\n\u001b[0;32m---> 76\u001b[0m     x_bb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     x_bb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x_bb\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_hidden_states :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_bb \u001b[38;5;241m+\u001b[39m x_pos\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[0;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    499\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m         output_attentions,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:355\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    348\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m ):\n\u001b[0;32m--> 355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    364\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:286\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    295\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:700\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    698\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    699\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_proj(query_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[0;32m--> 700\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose_for_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_proj(hidden_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[1;32m    703\u001b[0m rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kaggle-pii-5th-place-solution/.venv/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:657\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.transpose_for_scores\u001b[0;34m(self, x, attention_heads)\u001b[0m\n\u001b[1;32m    655\u001b[0m new_x_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (attention_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    656\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(new_x_shape)\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.train.train_utils import CollateFn, get_sampler, get_tokenizer\n",
    "\n",
    "oof_dfs = []\n",
    "tokenizer = get_tokenizer(config)\n",
    "collate_fn = CollateFn(tokenizer, is_train=True)\n",
    "\n",
    "for fold, (train_loader, valid_loader) in enumerate(dataloaders):\n",
    "    logger.info(f\"\\n FOLD{fold} : Training Start \\n\")\n",
    "\n",
    "    # First Training\n",
    "    trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "    best_score, best_steps, _ = trainer.train(train_loader, valid_loader)\n",
    "    break\n",
    "\n",
    "    # High Quality Data Training\n",
    "    # train_dataset = train_loader.dataset\n",
    "    # train_dataset.drop_first_only_data()\n",
    "    # train_loader = DataLoader(\n",
    "    #     train_dataset,\n",
    "    #     sampler=get_sampler(train_dataset),\n",
    "    #     batch_size=config.batch_size,\n",
    "    #     collate_fn=collate_fn,\n",
    "    #     pin_memory=True,\n",
    "    #     drop_last=True,\n",
    "    # )\n",
    "\n",
    "    # trainer = Trainer(config, logger, save_suffix=f\"_fold{fold}\")\n",
    "    # best_score, best_steps, oof_df = trainer.train(\n",
    "    #     train_loader,\n",
    "    #     valid_loader,\n",
    "    #     retrain=True,\n",
    "    #     retrain_weight_name=f\"model_fold{fold}_best\",\n",
    "    #     retrain_best_score=best_score,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train over folds\n",
    "oof_dfs = []\n",
    "best_steps_list, best_add_steps_list = [], []\n",
    "for fold, (train_loader, valid_loader) in enumerate(dataloaders):\n",
    "    logger.info(f\"\\n FOLD{fold} : Training Start \\n\")\n",
    "    model = get_model(config)\n",
    "    optimizer = get_optimizer(config, model)\n",
    "    oof_df, score, best_steps, best_add_steps = train_model(\n",
    "        config,\n",
    "        model,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        optimizer,\n",
    "        logger,\n",
    "        fold,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "    oof_df.write_parquet(config.oof_path / f\"oof_fold{fold}{suffix}.parquet\")\n",
    "    oof_dfs.append(oof_df)\n",
    "    best_score, best_th = get_best_negative_threshold(config, oof_df)\n",
    "    config.negative_th = best_th\n",
    "    message = f\"FOLD: {fold}, Steps: {best_steps} + {best_add_steps}, Best Score: {best_score}, Best Negative Threshold: {best_th}\"\n",
    "    logger.info(message)\n",
    "    best_steps_list.append(best_steps)\n",
    "    best_add_steps_list.append(best_add_steps)\n",
    "\n",
    "    del train_loader, valid_loader, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "del dataloaders\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save oof\n",
    "oof_df = pl.concat(oof_dfs)\n",
    "oof_df.write_parquet(config.oof_path / f\"oof_{config.exp}{suffix}.parquet\")\n",
    "\n",
    "# get best threshold\n",
    "best_score, best_th = get_best_negative_threshold(config, oof_df)\n",
    "message = f\"Overall OOF Best Score: {best_score}, Best Negative Threshold: {best_th}\"\n",
    "logger.info(message)\n",
    "config.negative_th = best_th\n",
    "\n",
    "# full train\n",
    "if config.full_train:\n",
    "    full_train_steps, full_train_add_steps = np.max(best_steps_list), np.max(best_add_steps_list)\n",
    "    logger.info(f\"\\n Full Train : Training Start, Num of Steps : {full_train_steps} + {full_train_add_steps}\\n\")\n",
    "    train_loader = get_full_train_loader(config, train_data)\n",
    "    model = get_model(config)\n",
    "    optimizer = get_optimizer(config, model)\n",
    "    full_train_model(config, model, train_loader, optimizer, full_train_steps, full_train_add_steps, logger, suffix)\n",
    "    message = \"Full Train Completed\"\n",
    "    logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
